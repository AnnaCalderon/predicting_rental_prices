{
  "hash": "c3827f5b4b433ea531a61b4ec4ec6c62",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Predicting Airbnb Rental Prices in NYC\"\nsubtitle: \"Building and Comparing Machine Learning Models for Price Forecasting\"\nformat:\n  html:\n    theme: cosmo\n    code-fold: true\n    toc: true\n    toc-depth: 3\n---\n\n## Overview\n\nThis project predicts nightly Airbnb rental prices in New York City using machine learning.  \nThe workflow includes:\n\n✅ **Data loading & preprocessing**  \n✅ **Feature engineering** (amenity extraction, zipcode aggregates, imputation)  \n✅ **Modeling using three algorithms:**\n   - Random Forest  \n   - Elastic Net  \n   - XGBoost  \n✅ **Model evaluation**  \n✅ **Model selection**  \n✅ **Diagnostic visualizations**\n\nThe final model can be used to generate price predictions for new, unseen listings.\n\n\n\n\n## 1. Load Libraries & Set global options\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\noptions(download.file.method = \"libcurl\")\n\n# Install guards (optional). Comment these out if your env already has them.\n# core_pkgs <- c(\"rlang\",\"cli\",\"vctrs\",\"pillar\",\"lifecycle\",\"openssl\")\n# try(suppressWarnings(install.packages(core_pkgs, type = \"binary\")), silent = TRUE)\n\nneed <- c(\"tidyverse\",\"janitor\",\"caret\",\"ranger\",\"glmnet\",\"xgboost\",\"rsample\",\"gt\",\"patchwork\",\"scales\")\nto_get <- need[!sapply(need, requireNamespace, quietly = TRUE)]\nif (length(to_get)) install.packages(to_get, type = \"binary\")\n\nsuppressPackageStartupMessages({\n  library(tidyverse)\n  library(janitor)\n  library(caret)\n  library(ranger)\n  library(glmnet)\n  library(xgboost)\n  library(rsample)\n  library(gt)\n  library(patchwork)\n  library(scales)\n})\n\nset.seed(5656)\n\n# ---- Global plot aesthetics ----\ntheme_set(theme_minimal(base_size = 12))\nupdate_geom_defaults(\"point\", list(alpha = 0.7))\n\nfmt_dollar <- scales::label_dollar(accuracy = 1, prefix = \"$\", big.mark = \",\", largest_with_cents = 100)\n```\n:::\n\n\n## 2. Data Cleaning & Feature Engineering\n\nData cleaning summary:\n\n- Missing beds are predicted using a regression model based on accommodates  \n- Zipcodes with missing values are imputed to the modal zipcode  \n- Amenities are separated and counted using tokenization  \n- Review ratings are normalized to a 0–5 scale  \n- Neighborhood price averages and medians are added without data leakage  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nanalysisData <- suppressWarnings(\n  readr::read_csv(\n    \"data/analysis_data.csv\",\n    col_types = cols(\n      id = col_integer(),\n      price = col_double(),\n      beds = col_double(),\n      bedrooms = col_double(),\n      bathrooms = col_double(),\n      accommodates = col_integer(),\n      review_scores_rating = col_double(),\n      number_of_reviews = col_integer(),\n      property_type = col_character(),\n      bed_type = col_character(),\n      neighbourhood_group_cleansed = col_character(),\n      room_type = col_character(),\n      cancellation_policy = col_character(),\n      host_is_superhost = col_character(),\n      instant_bookable  = col_character(),\n      zipcode = col_character(),\n      city = col_character(),\n      amenities = col_character()\n    )\n  )\n)\n\n\n\nscoring_exists <- file.exists(\"scoringData.csv\")\nscoringData <- if (scoring_exists) readr::read_csv(\"scoringData.csv\", show_col_types = FALSE) else NULL\n\n# Ensure common types BEFORE combining (zipcode as character)\n\nif (\"zipcode\" %in% names(analysisData)) analysisData <- analysisData %>% mutate(zipcode = as.character(zipcode))\nif (!is.null(scoringData) && \"zipcode\" %in% names(scoringData)) scoringData <- scoringData %>% mutate(zipcode = as.character(zipcode))\n```\n:::\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 70/30 split stratified by price\n\nsplit_obj <- rsample::initial_split(analysisData, prop = 0.70, strata = price)\ntrain <- rsample::training(split_obj)  %>% mutate(train_test_score = \"train\")\ntest  <- rsample::testing(split_obj)   %>% mutate(train_test_score = \"test\")\nif (!is.null(scoringData)) scoringData <- scoringData %>% mutate(train_test_score = \"score\")\n\n# Combine once; engineer once\n\nbaseData <- bind_rows(train, test, scoringData) %>% clean_names()\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Likely categoricals -> character (we'll factor later)\n\ncat_vars <- c(\"bed_type\",\"property_type\",\"neighbourhood_group_cleansed\",\"room_type\",\n\"cancellation_policy\",\"host_is_superhost\",\"instant_bookable\",\"zipcode\",\"city\")\nfor (v in intersect(cat_vars, names(baseData))) baseData[[v]] <- as.character(baseData[[v]])\n\n# 1) Beds imputation via accommodates (simple linear model)\n\nif (all(c(\"beds\",\"accommodates\") %in% names(baseData))) {\nfit_df <- baseData %>% filter(!is.na(beds), !is.na(accommodates))\nif (nrow(fit_df) > 10) {\nlm_beds <- lm(beds ~ accommodates, data = fit_df)\nbaseData <- baseData %>%\nmutate(\n  beds = if_else(\n    is.na(beds) & !is.na(accommodates),\n    round(predict(lm_beds, newdata = pick(everything())), 0),\n    beds\n  )\n)\n\n}\nif (any(is.na(baseData$beds))) {\nbaseData <- baseData %>% mutate(beds = if_else(is.na(beds), median(beds, na.rm = TRUE), beds))\n}\n}\n\n# 2) Zipcode impute to mode  ✅ FIXED\nif (\"zipcode\" %in% names(baseData)) {\n  zip_mode <- baseData %>%\n    dplyr::filter(!is.na(zipcode)) %>%\n    dplyr::count(zipcode, sort = TRUE) %>%\n    dplyr::slice_head(n = 1) %>%        # <= avoids the xgboost::slice conflict\n    dplyr::pull(zipcode)\n\n  baseData <- baseData %>% dplyr::mutate(zipcode = tidyr::replace_na(zipcode, zip_mode))\n}\n\n\n# 3) Amenities count\n\nif (all(c(\"id\",\"amenities\") %in% names(baseData))) {\namen_counts <- baseData %>%\nselect(id, amenities) %>%\nmutate(amenities = replace_na(amenities, \"\")) %>%\ntidyr::separate_rows(amenities, sep = \",\") %>%\nmutate(amenities = stringr::str_trim(amenities)) %>%\nfilter(amenities != \"\") %>%\ncount(id, name = \"count_amenities\")\nbaseData <- baseData %>%\nleft_join(amen_counts, by = \"id\") %>%\nmutate(count_amenities = replace_na(count_amenities, 0L))\n}\n\n# 4) Normalized review score (0–5 from 0–100)\n\nbaseData <- baseData %>% mutate(review_score_5 = if (\"review_scores_rating\" %in% names(.)) review_scores_rating/20 else NA_real_)\n\n# Re-split to keep row roles\n\ntrain <- baseData %>% filter(train_test_score == \"train\")\ntest  <- baseData %>% filter(train_test_score == \"test\")\nscore <- baseData %>% filter(train_test_score == \"score\") %>% { if (nrow(.) == 0) NULL else . }\n\n# Zipcode aggregates (TRAIN ONLY) to avoid leakage\n\nif (all(c(\"zipcode\",\"price\") %in% names(train))) {\nzip_aggs <- train %>%\ngroup_by(zipcode) %>%\nsummarise(zip_price_median = median(price, na.rm = TRUE),\nzip_price_mean   = mean(price,   na.rm = TRUE),\n.groups = \"drop\")\nadd_zip <- function(df) {\nout <- df %>% left_join(zip_aggs, by = \"zipcode\")\nout %>%\nmutate(zip_price_median = replace_na(zip_price_median, median(train$price, na.rm = TRUE)),\nzip_price_mean   = replace_na(zip_price_mean,   mean(train$price, na.rm = TRUE)))\n}\ntrain <- add_zip(train); test <- add_zip(test); if (!is.null(score)) score <- add_zip(score)\n}\n```\n:::\n\n## 3. Exploratory Data Analysis (EDA)\nPrice distributions show heavy right skew, which motivates log-transformation during modeling. Room type and capacity also show clear price separation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\np1 <- ggplot(train, aes(price)) +\ngeom_histogram(bins = 50) +\nscale_x_continuous(labels = fmt_dollar) +\nlabs(title = \"Price distribution\", x = \"Price (USD)\", y = \"Count\")\n\np2 <- ggplot(train, aes(log1p(price))) +\ngeom_histogram(bins = 50) +\nlabs(title = \"Log-price distribution\", x = \"log1p(Price)\", y = \"Count\")\n\np3 <- train %>%\nfilter(!is.na(room_type)) %>%\nggplot(aes(x = room_type, y = price)) +\ngeom_boxplot(outlier.alpha = 0.2) +\nscale_y_continuous(labels = fmt_dollar) +\nlabs(title = \"Price by room type\", x = \"\", y = \"Price (USD)\")\n\np4 <- ggplot(train, aes(x = accommodates, y = price)) +\ngeom_point(size = 0.9) +\ngeom_smooth(se = FALSE) +\nscale_y_continuous(labels = fmt_dollar) +\nlabs(title = \"Price vs. accommodates\", x = \"Accommodates\", y = \"Price (USD)\")\n\n(p1 | p2) / (p3 | p4)\n```\n\n::: {.cell-output-display}\n![](about_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n## 4. Modeling\n\nModels are trained on **log(price)** to stabilize variance and penalize large errors.\nCategorical variables are one-hot encoded and numeric variables are median-imputed.\n\nModels trained:\n\n| Model | Library | Notes |\n|-------|---------|-------|\n| Random Forest | ranger | Non-linear, robust, interpretable feature importance |\n| Elastic Net | glmnet | Linear, with L1/L2 regularization |\n| XGBoost | xgboost | Gradient boosting, strong performance on tabular data |\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ---- Target on log scale ----\n\ntrain <- train %>% mutate(price_log = log1p(price))\ntest  <- test  %>% mutate(price_log = log1p(price))\n\n# Predictors (keep those present)\n\npredictors <- c(\"host_is_superhost\",\"beds\",\"count_amenities\",\"neighbourhood_group_cleansed\",\"room_type\",\n\"accommodates\",\"bathrooms\",\"bedrooms\",\"cancellation_policy\",\"review_score_5\",\n\"number_of_reviews\",\"property_type\",\"instant_bookable\",\n\"zip_price_median\",\"zip_price_mean\",\"city\",\"zipcode\")\npredictors <- intersect(predictors, names(train))\n\n# Numeric vs categorical\n\nis_num <- vapply(train[, predictors, drop = FALSE], is.numeric, logical(1))\nnum_vars <- names(is_num[is_num])\ncat_vars_model <- setdiff(predictors, num_vars)\n\n# Numeric: median-impute using TRAIN\n\nnum_medians <- purrr::map_dbl(num_vars, ~ median(train[[.x]], na.rm = TRUE))\nfor (v in num_vars) {\nif (anyNA(train[[v]])) train[[v]][is.na(train[[v]])] <- num_medians[[v]]\nif (anyNA(test[[v]]))  test[[v]][is.na(test[[v]])]  <- num_medians[[v]]\n}\n\n# Categorical: NA -> \"Unknown\"; rare (<1%) -> \"Other\" based on TRAIN\n\npool_rare <- function(x_train, x_new = NULL, thresh = 0.01) {\ntx <- ifelse(is.na(x_train), \"Unknown\", as.character(x_train))\nprops <- prop.table(table(tx))\nkeep <- names(props[props >= thresh])\nmap_levels <- function(z) {\nz <- ifelse(is.na(z), \"Unknown\", as.character(z))\nz[!(z %in% keep) & z != \"Unknown\"] <- \"Other\"\nfactor(z, levels = unique(c(keep, \"Other\", \"Unknown\")))\n}\nlist(train = map_levels(tx), new = if (!is.null(x_new)) map_levels(x_new) else NULL)\n}\nfor (v in cat_vars_model) {\nres <- pool_rare(train[[v]], test[[v]], thresh = 0.01)\ntrain[[v]] <- res$train; test[[v]] <- res$new\n}\nfor (v in cat_vars_model) {\ntrain[[v]] <- as.factor(train[[v]])\ntest[[v]]  <- factor(test[[v]], levels = levels(train[[v]]))\n}\n\n# Model matrices (one-hot)\n\ndf_train <- train %>% dplyr::select(all_of(c(\"price_log\", predictors)))\ndf_test  <- test  %>% dplyr::select(all_of(c(\"price_log\", predictors)))\n\nX_train <- model.matrix(price_log ~ . - 1, data = df_train)\nX_test  <- model.matrix(price_log ~ . - 1, data = df_test)\n\n# Align columns\n\nmissing_in_test <- setdiff(colnames(X_train), colnames(X_test))\nif (length(missing_in_test)) {\nX_test <- cbind(X_test, matrix(0, nrow = nrow(X_test), ncol = length(missing_in_test),\ndimnames = list(NULL, missing_in_test)))\n}\nextra_in_test <- setdiff(colnames(X_test), colnames(X_train))\nif (length(extra_in_test)) {\nX_test <- X_test[, setdiff(colnames(X_test), extra_in_test), drop = FALSE]\n}\nX_test <- X_test[, colnames(X_train), drop = FALSE]\n\n# Targets\n\ny_train <- df_train$price_log\ny_test  <- df_test$price_log\n\n# CV control (used only if you retrain later)\n\ncv_ctrl <- caret::trainControl(method = \"repeatedcv\", number = 5, repeats = 2, verboseIter = FALSE, allowParallel = TRUE)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ### ---------------- Random Forest (caret + ranger, saved model) ----------------\n# if (!dir.exists(\"models\")) dir.create(\"models\")\n# model_path_rf <- \"models/rf_fit.rds\"\n# \n# if (file.exists(model_path_rf)) {\n#   message(\"Loading saved Random Forest model...\")\n#   rf_fit <- readRDS(model_path_rf)\n# } else {\n#   message(\"Training Random Forest model (first time)...\")\n#   \n#   p <- ncol(X_train)\n#   rf_grid <- expand.grid(\n#     mtry          = unique(pmax(2, floor(c(sqrt(p), sqrt(p)*1.5, sqrt(p)*2)))),\n#     splitrule     = c(\"variance\", \"extratrees\"),\n#     min.node.size = c(3, 5, 10)\n#   )\n# \n#   set.seed(5656)\n#   rf_fit <- caret::train(\n#     x = X_train,\n#     y = y_train,\n#     method      = \"ranger\",\n#     trControl   = cv_ctrl,\n#     tuneGrid    = rf_grid,\n#     num.trees   = 1000,\n#     importance  = \"impurity\",\n#     metric      = \"RMSE\"\n#   )\n#   \n#   saveRDS(rf_fit, model_path_rf)\n#   message(\"Saved Random Forest model to \", model_path_rf)\n# }\n# \n# # ---- Evaluate on test set ----\n# rf_pred_log   <- predict(model_path_rf, newdata = X_test)\n# rf_pred_price <- pmax(expm1(rf_pred_log), 0)\n# \n# # yardstick metrics if available; else caret::postResample\n# if (requireNamespace(\"yardstick\", quietly = TRUE)) {\n#   rf_metrics <- tibble::tibble(price = expm1(y_test), .pred = rf_pred_price) |>\n#     yardstick::metrics(truth = price, estimate = .pred) |>\n#     dplyr::filter(.metric %in% c(\"rmse\",\"rsq\"))\n# } else {\n#   pm <- caret::postResample(pred = rf_pred_price, obs = expm1(y_test))\n#   rf_metrics <- data.frame(.metric = c(\"rmse\",\"rsq\"),\n#                            .estimate = c(unname(pm[\"RMSE\"]), unname(pm[\"Rsquared\"])))\n# }\n# rf_metrics\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ### ---------------- Elastic Net (caret + glmnet, saved model) ----------------\n# if (!dir.exists(\"models\")) dir.create(\"models\")\n# model_path_en <- \"models/en_fit.rds\"\n# \n# if (file.exists(model_path_en)) {\n#   message(\"Loading saved Elastic Net model...\")\n#   en_fit <- readRDS(model_path_en)\n# } else {\n#   message(\"Training Elastic Net model (first time)...\")\n#   \n#   # Tune across a broad but sensible grid\n#   en_grid <- expand.grid(\n#     alpha  = seq(0, 1, by = 0.25),                    # ridge (0) -> lasso (1)\n#     lambda = 10 ^ seq(2, -4, length.out = 50)         # regularization strength\n#   )\n#   \n#   set.seed(5656)\n#   en_fit <- caret::train(\n#     x = X_train,\n#     y = y_train,\n#     method      = \"glmnet\",\n#     trControl   = cv_ctrl,        # 5x2 repeated CV from your prep\n#     tuneGrid    = en_grid,\n#     standardize = TRUE            # glmnet standardization (recommended)\n#     # preProcess = c(\"center\",\"scale\")  # optional, but glmnet standardizes internally\n#   )\n#   \n#   saveRDS(en_fit, model_path_en)\n#   message(\"Saved Elastic Net model to \", model_path_en)\n# }\n# \n# # ---- Evaluate on test set ----\n# en_pred_log   <- predict(en_fit, newdata = X_test)\n# en_pred_price <- pmax(expm1(en_pred_log), 0)\n# \n# # yardstick metrics if available; else caret::postResample\n# if (requireNamespace(\"yardstick\", quietly = TRUE)) {\n#   en_metrics <- tibble::tibble(price = expm1(y_test), .pred = en_pred_price) |>\n#     yardstick::metrics(truth = price, estimate = .pred) |>\n#     dplyr::filter(.metric %in% c(\"rmse\",\"rsq\"))\n# } else {\n#   pm <- caret::postResample(pred = en_pred_price, obs = expm1(y_test))\n#   en_metrics <- data.frame(.metric = c(\"rmse\",\"rsq\"),\n#                            .estimate = c(unname(pm[\"RMSE\"]), unname(pm[\"Rsquared\"])))\n# }\n# en_metrics\n\n# ---- (Optional) Top coefficients for portfolio plot ----\n# Extract coefficients at best alpha/lambda\n# best_a <- en_fit$bestTune$alpha\n# best_l <- en_fit$bestTune$lambda\n# coefs  <- as.matrix(coef(en_fit$finalModel, s = best_l))\n# coef_tbl <- tibble::tibble(\n#   term = rownames(coefs),\n#   estimate = as.numeric(coefs)\n# ) |>\n#   dplyr::filter(term != \"(Intercept)\") |>\n#   dplyr::arrange(dplyr::desc(abs(estimate)))\n# \n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ### ---------------- XGBoost (caret + xgboost, saved model) ----------------\n# if (!dir.exists(\"models\")) dir.create(\"models\")\n# model_path_xgb <- \"models/xgb_fit.rds\"\n# \n# if (file.exists(model_path_xgb)) {\n#   message(\"Loading saved XGBoost model...\")\n#   xgb_fit <- readRDS(model_path_xgb)\n# } else {\n#   message(\"Training XGBoost model (first time)...\")\n#   \n#   # caret::xgbTree tuning grid (balanced for speed/quality)\n#   xgb_grid <- expand.grid(\n#     nrounds = c(400, 800, 1200),     # boosting iterations\n#     max_depth = c(4, 6, 8),          # tree depth\n#     eta = c(0.03, 0.1),              # learning rate\n#     gamma = c(0, 1),                 # minimum loss reduction\n#     colsample_bytree = c(0.6, 0.8),  # column subsample\n#     min_child_weight = c(1, 3),      # min sum hessian in child\n#     subsample = c(0.7, 1.0)          # row subsample\n#   )\n# \n#   set.seed(5656)\n#   xgb_fit <- caret::train(\n#     x = X_train,\n#     y = y_train,\n#     method    = \"xgbTree\",\n#     trControl = cv_ctrl,        # your 5x2 repeated CV\n#     tuneGrid  = xgb_grid,\n#     metric    = \"RMSE\",\n#     verbose   = FALSE\n#   )\n#   \n#   saveRDS(xgb_fit, model_path_xgb)\n#   message(\"Saved XGBoost model to \", model_path_xgb)\n# }\n# \n# # ---- Evaluate on test set ---- \n# xgb_pred_log   <- predict(xgb_fit, newdata = X_test)\n# xgb_pred_price <- pmax(expm1(xgb_pred_log), 0)\n# \n# # yardstick if available; else use caret::postResample\n# if (requireNamespace(\"yardstick\", quietly = TRUE)) {\n#   xgb_metrics <- tibble::tibble(price = expm1(y_test), .pred = xgb_pred_price) |>\n#     yardstick::metrics(truth = price, estimate = .pred) |>\n#     dplyr::filter(.metric %in% c(\"rmse\",\"rsq\"))\n# } else {\n#   pm <- caret::postResample(pred = xgb_pred_price, obs = expm1(y_test))\n#   xgb_metrics <- data.frame(.metric = c(\"rmse\",\"rsq\"),\n#                             .estimate = c(unname(pm[\"RMSE\"]), unname(pm[\"Rsquared\"])))\n# }\n# xgb_metrics\n# \n# # ---- (Optional) Feature importance (top 20) ----\n# # Uses the underlying xgboost booster inside the caret model\n# if (requireNamespace(\"xgboost\", quietly = TRUE)) {\n#   imp <- xgboost::xgb.importance(feature_names = colnames(X_train),\n#                                  model = xgb_fit$finalModel)\n# }\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ---- Load saved models from models/ ----\n\nmodel_path_rf  <- \"models/rf_fit.rds\"\nmodel_path_en  <- \"models/en_fit.rds\"\nmodel_path_xgb <- \"models/xgb_fit.rds\"\n\nrf_fit  <- if (file.exists(model_path_rf))  readRDS(model_path_rf)  else NULL\nen_fit  <- if (file.exists(model_path_en))  readRDS(model_path_en)  else NULL\nxgb_fit <- if (file.exists(model_path_xgb)) readRDS(model_path_xgb) else NULL\n\n# Helpers\n\nbt <- function(x) pmax(expm1(x), 0)  # back-transform from log1p\neval_model <- function(fit, X_new, y_true_log) {\nstopifnot(!is.null(fit))\npred_log <- predict(fit, newdata = X_new)\npred     <- bt(as.numeric(pred_log))\nobs      <- bt(as.numeric(y_true_log))\npreds_df <- tibble::tibble(price = obs, .pred = pred, resid = obs - pred)\n\nif (requireNamespace(\"yardstick\", quietly = TRUE)) {\nmets <- preds_df |>\nyardstick::metrics(truth = price, estimate = .pred) |>\ndplyr::filter(.metric %in% c(\"rmse\",\"rsq\"))\n} else {\npm <- caret::postResample(pred = preds_df$.pred, obs = preds_df$price)\nmets <- tibble::tibble(.metric = c(\"rmse\",\"rsq\"),\n.estimate = c(unname(pm[\"RMSE\"]), unname(pm[\"Rsquared\"])))\n}\nlist(preds = preds_df, metrics = mets)\n}\n```\n:::\n\n\n\n\n\n## 5. Model Comparison & Selection\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbt <- function(x) pmax(expm1(x), 0)\n\n# Align a data.frame version of X to the model's required predictor names\nalign_newdata <- function(fit, X) {\n  df <- as.data.frame(X)\n  # Try to get the predictor names the model expects\n  req <- NULL\n  # ranger via caret\n  if (!is.null(fit$finalModel) && inherits(fit$finalModel, \"ranger\")) {\n    req <- try(fit$finalModel$forest$independent.variable.names, silent = TRUE)\n  }\n  # generic caret fallback (uses trainingData columns, minus .outcome)\n  if (is.null(req) || inherits(req, \"try-error\")) {\n    if (!is.null(fit$trainingData)) {\n      req <- setdiff(colnames(fit$trainingData), \".outcome\")\n    }\n  }\n  # Final fallback: just use colnames(X) (shouldn't happen, but keeps robustness)\n  if (is.null(req)) req <- colnames(df)\n\n  # Add missing as zeros, drop extras, order to req\n  miss <- setdiff(req, colnames(df))\n  if (length(miss)) for (m in miss) df[[m]] <- 0\n  extra <- setdiff(colnames(df), req)\n  if (length(extra)) df <- df[, setdiff(colnames(df), extra), drop = FALSE]\n  df[, req, drop = FALSE]\n}\n\n# Robust evaluator: try aligned X first; then raw test df as fallback\neval_model <- function(fit, X_test, y_test, test_df) {\n  pred_log <- NULL\n\n  nd <- try(align_newdata(fit, X_test), silent = TRUE)\n  if (!inherits(nd, \"try-error\")) {\n    pred_log <- try(predict(fit, newdata = nd), silent = TRUE)\n  }\n  if (inherits(pred_log, \"try-error\") || is.null(pred_log)) {\n    pred_log <- try(predict(fit, newdata = test_df), silent = TRUE)\n  }\n  if (inherits(pred_log, \"try-error\") || is.null(pred_log)) return(NULL)\n\n  preds <- tibble::tibble(price = expm1(y_test), .pred = bt(pred_log))\n  list(\n    preds   = preds,\n    metrics = preds |>\n      yardstick::metrics(truth = price, estimate = .pred) |>\n      dplyr::filter(.metric %in% c(\"rmse\",\"rsq\"))\n  )\n}\n\n# ---- Evaluate whatever models are available ----\nresults <- list()\nif (exists(\"rf_fit\")  && !is.null(rf_fit))  results$RF  <- eval_model(rf_fit,  X_test, y_test, test)\nif (exists(\"en_fit\")  && !is.null(en_fit))  results$EN  <- eval_model(en_fit,  X_test, y_test, test)\nif (exists(\"xgb_fit\") && !is.null(xgb_fit)) results$XGB <- eval_model(xgb_fit, X_test, y_test, test)\n\n# Keep only successful evaluations\nresults <- purrr::compact(results)\nstopifnot(length(results) > 0)\n\n# ---- Build comparison table ----\nmetrics_tbl <- purrr::imap_dfr(results, ~ .x$metrics %>% dplyr::mutate(model = .y)) %>%\n  dplyr::select(model, .metric, .estimate) %>%\n  dplyr::mutate(.metric = tolower(.metric)) %>%\n  tidyr::pivot_wider(names_from = .metric, values_from = .estimate) %>%\n  dplyr::mutate(\n    rmse = as.numeric(rmse),\n    rsq  = as.numeric(rsq)\n  ) %>%\n  dplyr::arrange(rmse)\n\n# Optional pretty table\nmetrics_tbl %>%\n  gt::gt() %>%\n  gt::tab_header(title = \"Model Performance on Test Set\") %>%\n  gt::cols_label(rmse = \"RMSE ($)\", rsq = \"R²\") %>%\n  gt::tab_style(style = gt::cell_text(weight = \"bold\"),\n                locations = gt::cells_column_labels())\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"szoniisfty\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n<style>#szoniisfty table {\n  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';\n  -webkit-font-smoothing: antialiased;\n  -moz-osx-font-smoothing: grayscale;\n}\n\n#szoniisfty thead, #szoniisfty tbody, #szoniisfty tfoot, #szoniisfty tr, #szoniisfty td, #szoniisfty th {\n  border-style: none;\n}\n\n#szoniisfty p {\n  margin: 0;\n  padding: 0;\n}\n\n#szoniisfty .gt_table {\n  display: table;\n  border-collapse: collapse;\n  line-height: normal;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#szoniisfty .gt_caption {\n  padding-top: 4px;\n  padding-bottom: 4px;\n}\n\n#szoniisfty .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#szoniisfty .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 3px;\n  padding-bottom: 5px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#szoniisfty .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#szoniisfty .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#szoniisfty .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#szoniisfty .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#szoniisfty .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#szoniisfty .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#szoniisfty .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#szoniisfty .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 5px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#szoniisfty .gt_spanner_row {\n  border-bottom-style: hidden;\n}\n\n#szoniisfty .gt_group_heading {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  text-align: left;\n}\n\n#szoniisfty .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#szoniisfty .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#szoniisfty .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#szoniisfty .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#szoniisfty .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#szoniisfty .gt_stub_row_group {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n  vertical-align: top;\n}\n\n#szoniisfty .gt_row_group_first td {\n  border-top-width: 2px;\n}\n\n#szoniisfty .gt_row_group_first th {\n  border-top-width: 2px;\n}\n\n#szoniisfty .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#szoniisfty .gt_first_summary_row {\n  border-top-style: solid;\n  border-top-color: #D3D3D3;\n}\n\n#szoniisfty .gt_first_summary_row.thick {\n  border-top-width: 2px;\n}\n\n#szoniisfty .gt_last_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#szoniisfty .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#szoniisfty .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#szoniisfty .gt_last_grand_summary_row_top {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: double;\n  border-bottom-width: 6px;\n  border-bottom-color: #D3D3D3;\n}\n\n#szoniisfty .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#szoniisfty .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#szoniisfty .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#szoniisfty .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#szoniisfty .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#szoniisfty .gt_sourcenote {\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#szoniisfty .gt_left {\n  text-align: left;\n}\n\n#szoniisfty .gt_center {\n  text-align: center;\n}\n\n#szoniisfty .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#szoniisfty .gt_font_normal {\n  font-weight: normal;\n}\n\n#szoniisfty .gt_font_bold {\n  font-weight: bold;\n}\n\n#szoniisfty .gt_font_italic {\n  font-style: italic;\n}\n\n#szoniisfty .gt_super {\n  font-size: 65%;\n}\n\n#szoniisfty .gt_footnote_marks {\n  font-size: 75%;\n  vertical-align: 0.4em;\n  position: initial;\n}\n\n#szoniisfty .gt_asterisk {\n  font-size: 100%;\n  vertical-align: 0;\n}\n\n#szoniisfty .gt_indent_1 {\n  text-indent: 5px;\n}\n\n#szoniisfty .gt_indent_2 {\n  text-indent: 10px;\n}\n\n#szoniisfty .gt_indent_3 {\n  text-indent: 15px;\n}\n\n#szoniisfty .gt_indent_4 {\n  text-indent: 20px;\n}\n\n#szoniisfty .gt_indent_5 {\n  text-indent: 25px;\n}\n\n#szoniisfty .katex-display {\n  display: inline-flex !important;\n  margin-bottom: 0.75em !important;\n}\n\n#szoniisfty div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after {\n  height: 0px !important;\n}\n</style>\n<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n  <thead>\n    <tr class=\"gt_heading\">\n      <td colspan=\"3\" class=\"gt_heading gt_title gt_font_normal gt_bottom_border\" style>Model Performance on Test Set</td>\n    </tr>\n    \n    <tr class=\"gt_col_headings\">\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" style=\"font-weight: bold;\" scope=\"col\" id=\"model\">model</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" style=\"font-weight: bold;\" scope=\"col\" id=\"rmse\">RMSE ($)</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" style=\"font-weight: bold;\" scope=\"col\" id=\"rsq\">R²</th>\n    </tr>\n  </thead>\n  <tbody class=\"gt_table_body\">\n    <tr><td headers=\"model\" class=\"gt_row gt_left\">XGB</td>\n<td headers=\"rmse\" class=\"gt_row gt_right\">65.65469</td>\n<td headers=\"rsq\" class=\"gt_row gt_right\">0.6414969</td></tr>\n    <tr><td headers=\"model\" class=\"gt_row gt_left\">RF</td>\n<td headers=\"rmse\" class=\"gt_row gt_right\">66.96682</td>\n<td headers=\"rsq\" class=\"gt_row gt_right\">0.6360707</td></tr>\n    <tr><td headers=\"model\" class=\"gt_row gt_left\">EN</td>\n<td headers=\"rmse\" class=\"gt_row gt_right\">70.17521</td>\n<td headers=\"rsq\" class=\"gt_row gt_right\">0.5860496</td></tr>\n  </tbody>\n  \n  \n</table>\n</div>\n```\n\n:::\n\n```{.r .cell-code}\n# ---- Pick best & prep for visuals ----\nbest_name <- metrics_tbl$model[1]\nbest_preds <- results[[best_name]]$preds %>%\n  dplyr::transmute(price, pred = .pred, resid = price - .pred)\n\nmessage(sprintf(\"✅ Best model: %s — RMSE $%.1f | R² %.3f\",\n                best_name, metrics_tbl$rmse[1], metrics_tbl$rsq[1]))\n\nbest_name <- as.character(metrics_tbl$model[1])\n\n# Map the winning label -> actual model object\nbest_fit <- switch(best_name,\n  RF  = rf_fit,\n  EN  = en_fit,\n  XGB = xgb_fit\n)\nstopifnot(!is.null(best_fit))\n```\n:::\n\nThe table shows performance on the held-out test set.  \nLower RMSE = better. Higher R² = better.\n\n\nThe best-performing model is **XGB**, with:\n\n- **RMSE:** $65.70\n- **R²:** 0.641\n\nThis model is used for all diagnostics and predictions.\n\n---\n\n## 6. Model Diagnostics\nInterpretation of Diagnostic Plots\n\nPredicted vs Actual:\nThe XGBoost model closely tracks the 45° reference line, indicating good predictive accuracy. Most points cluster in the lower–mid price range, with some under-prediction at higher prices — a common pattern when extreme nightly rates are rare in the training data.\n\nResiduals vs Predicted:\nResiduals are centered around zero with no strong patterns, suggesting the model is not systematically over- or under-predicting across most price ranges. A few large positive and negative residuals indicate outliers (listings with unusually high or low prices).\n\nResiduals Q–Q Plot:\nThe residual distribution deviates from perfect normality in the tails — again driven by extreme listings — but the majority fall near the reference line, supporting that model errors are approximately symmetric and well-behaved.\n\nOverall, the diagnostics show that the XGBoost model generalizes well, captures the main pricing structure in the data, and only struggles on a small number of extreme cases (luxury or atypical listings).\n\n::: {.cell}\n\n```{.r .cell-code}\n# --- prerequisites ---\nif (!exists(\"metrics_tbl\")) stop(\"metrics_tbl not found.\")\nif (!exists(\"results\"))     stop(\"results not found.\")\nif (!requireNamespace(\"hexbin\", quietly = TRUE)) install.packages(\"hexbin\")\nif (!requireNamespace(\"patchwork\", quietly = TRUE)) install.packages(\"patchwork\")\nlibrary(patchwork)\n\n# pick best model (lowercase metric names)\nbest_row  <- metrics_tbl %>% dplyr::slice_min(rmse, n = 1)\nbest_name <- as.character(best_row$model)\n\n# (re)build best_preds if needed\nif (!exists(\"best_preds\") || is.null(best_preds)) {\n  if (!is.null(results[[best_name]]) && !is.null(results[[best_name]]$preds)) {\n    best_preds <- results[[best_name]]$preds %>%\n      dplyr::transmute(price, pred = .pred, resid = price - .pred)\n  } else {\n    if (!exists(\"bt\")) bt <- function(x) pmax(expm1(x), 0)\n    pred_log <- predict(switch(best_name, RF=rf_fit, EN=en_fit, XGB=xgb_fit), newdata = X_test)\n    best_preds <- tibble::tibble(\n      price = bt(as.numeric(y_test)),\n      pred  = bt(as.numeric(pred_log))\n    ) %>% dplyr::mutate(resid = price - pred)\n  }\n}\n\n# helpers\nif (!exists(\"fmt_dollar\")) fmt_dollar <- scales::label_dollar(accuracy = 1, prefix = \"$\", big.mark = \",\")\n\n# set symmetric residual limits (trim extreme outliers for readability)\nr_lim <- stats::quantile(abs(best_preds$resid), 0.99, na.rm = TRUE) |> as.numeric()\n\n# --- Plot A: Predicted vs Actual (bigger at top) ---\npA <- ggplot(best_preds, aes(x = price, y = pred)) +\n  geom_hex(bins = 35, show.legend = TRUE) +\n  scale_fill_viridis_c(name = \"Count\") +\n  geom_abline(slope = 1, intercept = 0, linetype = 2, linewidth = 0.7) +\n  coord_equal() +\n  scale_x_continuous(labels = fmt_dollar, breaks = scales::breaks_pretty(6)) +\n  scale_y_continuous(labels = fmt_dollar, breaks = scales::breaks_pretty(6)) +\n  labs(title = sprintf(\"%s — Predicted vs Actual\", best_name),\n       x = \"Actual price\", y = \"Predicted price\") +\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    legend.position = \"right\"\n  )\n\n# --- Plot B: Residuals vs Predicted ---\npB <- ggplot(best_preds, aes(x = pred, y = resid)) +\n  geom_point(alpha = 0.25, size = 0.8) +\n  geom_smooth(method = \"loess\", se = FALSE, linewidth = 0.9) +\n  geom_hline(yintercept = 0, linetype = 2, linewidth = 0.6) +\n  scale_x_continuous(labels = fmt_dollar, breaks = scales::breaks_pretty(5)) +\n  scale_y_continuous(limits = c(-r_lim, r_lim)) +\n  labs(title = \"Residuals vs Predicted\",\n       x = \"Predicted price\", y = \"Residual\") +\n  theme_minimal(base_size = 12) +\n  theme(plot.title = element_text(face = \"bold\"))\n\n# --- Plot C: Residuals Q–Q Plot ---\npC <- ggplot(best_preds, aes(sample = resid)) +\n  stat_qq(alpha = 0.5, size = 1) +\n  stat_qq_line(color = \"red\", linewidth = 0.8) +\n  labs(title = \"Residuals — Q–Q Plot\",\n       x = \"Theoretical Quantiles\", y = \"Sample Quantiles\") +\n  theme_minimal(base_size = 12) +\n  theme(plot.title = element_text(face = \"bold\"))\n# Add subtle margins so labels never collide with the panel edge\npA <- pA + theme(plot.margin = margin(6, 12, 6, 6))\npB <- pB + theme(plot.margin = margin(6, 12, 6, 6))\npC <- pC + theme(plot.margin = margin(6, 6, 6, 12))\n\n# Build a spacer to force separation\nsp <- patchwork::plot_spacer()\n\n# Bottom row with a spacer column in the middle\nbottom_row <- (pB | sp | pC) + patchwork::plot_layout(widths = c(1, 0.08, 1))\n\n# Final layout: large top, separated bottoms\nfinal_diag <- (pA) / bottom_row +\n  patchwork::plot_layout(heights = c(1.35, 1), guides = \"collect\") &\n  theme(legend.position = \"right\")\n\nfinal_diag\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 119 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 119 rows containing missing values or values outside the scale range\n(`geom_point()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](about_files/figure-html/diagnostics-1.png){width=1056}\n:::\n:::\n\n\n\n\n## 7. Feature Importance\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Ensure best_fit exists even if this chunk is run alone\nif (!exists(\"best_fit\") || is.null(best_fit)) {\n  best_fit <- switch(best_name, RF = rf_fit, EN = en_fit, XGB = xgb_fit)\n  stopifnot(!is.null(best_fit))\n}\n\nif (best_name == \"RF\") {\n\n# caret::varImp works on the trained ranger model wrapped by caret\n\nvi <- caret::varImp(best_fit)$importance %>%\ntibble::rownames_to_column(\"feature\") %>%\ndplyr::arrange(dplyr::desc(Overall)) %>%\ndplyr::slice(1:20)\nggplot(vi, aes(x = reorder(feature, Overall), y = Overall)) +\ngeom_col(fill = \"#4E79A7\") + coord_flip() +\nlabs(title = \"Top 20 Features — Random Forest\", x = \"\", y = \"Importance\")\n\n} else if (best_name == \"XGB\") {\n\n# Use underlying xgboost booster inside caret model\n\nif (requireNamespace(\"xgboost\", quietly = TRUE)) {\nimp <- xgboost::xgb.importance(\nfeature_names = colnames(X_train),\nmodel = best_fit$finalModel\n)\nimp <- imp %>% dplyr::slice(1:20)\nggplot(imp, aes(x = reorder(Feature, Gain), y = Gain)) +\ngeom_col(fill = \"#E67E22\") + coord_flip() +\nlabs(title = \"Top 20 Features — XGBoost\", x = \"\", y = \"Gain\")\n}\n} else if (best_name == \"EN\") {\n\n# Coefficients from glmnet at best lambda\n\nbest_l <- best_fit$bestTune$lambda\ncf <- as.matrix(coef(best_fit$finalModel, s = best_l))\nen_coefs <- tibble::tibble(term = rownames(cf), estimate = as.numeric(cf)) %>%\ndplyr::filter(term != \"(Intercept)\") %>%\ndplyr::arrange(dplyr::desc(abs(estimate))) %>%\ndplyr::slice(1:20)\nggplot(en_coefs, aes(x = reorder(term, estimate), y = estimate)) +\ngeom_col(fill = \"#2E86C1\") + coord_flip() +\nlabs(title = \"Top 20 Elastic Net Coefficients\", x = \"\", y = \"Coefficient (log-price scale)\")\n}\n```\n\n::: {.cell-output-display}\n![](about_files/figure-html/importance-or-coefs-1.png){width=672}\n:::\n:::\n\n\n::: callout-note\n**Model-specific interpretation notes**\n\n- **XGBoost “Gain”**\n  - Gain tells us how important a feature is by measuring how much it helps the model reduce prediction error.\n  - Higher gain = the feature contributed more to improving accuracy.\n\n- **Elastic Net coefficients**\n  - Coefficients are learned on the log-price scale, which means they translate to percentage changes in price.\n  - To interpret a coefficient: convert using `exp(coef) - 1`.\n  - Example: a coefficient of **+0.07** ≈ **+7.25%** higher price, because `exp(0.07) - 1 ≈ 0.0725`.\n:::\n\n\n\n## 8. Conclusion\n\nThis project successfully built and compared three machine learning models to predict nightly Airbnb prices in New York City.  \nAfter evaluating performance on a held-out test set, **XGBoost** achieved the best results, with the lowest RMSE and highest R², indicating strong predictive accuracy and generalization.\n\nKey takeaways:\n\n- Prices are strongly driven by room type, accommodates capacity, amenities, and neighborhood-level pricing.\n- The model performs well across the majority of listings, with only a small number of outlier properties showing large residuals (typically luxury or highly atypical units).\n- Diagnostics confirm that model errors are centered around zero and mostly symmetric, suggesting a stable and unbiased fit.\n- Feature importance aligns with intuition: higher capacity, prime neighborhoods, and better amenities are associated with higher nightly rates.\n\n### What could improve future performance?\n\n- Include calendar/seasonality features (weekends, holidays, peak tourism months)\n- Incorporate host reputation (e.g., number of past stays, cancellation rates, response time)\n- Fit a model that accounts for geographic distance to landmarks (Times Square, major subway hubs)\n- Try advanced boosting approaches (LightGBM, CatBoost) or stacking models\n- Model price as a **two-step process**:  \n  1) Predict if a listing is luxury / high-tier  \n  2) Predict price within each tier\n\n### Final note:\n\nThe XGBoost model can now be applied to **new, unseen listings** to provide estimated nightly prices, making it useful for hosts, investors, and platforms evaluating pricing strategies.\n\n\n\n\n",
    "supporting": [
      "about_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}