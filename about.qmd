---
title: "Predicting Airbnb Rental Prices in NYC"
subtitle: "Building and Comparing Machine Learning Models for Price Forecasting"
format:
  html:
    theme: cosmo
    code-fold: true
    toc: true
    toc-depth: 3
---

## Overview

This project predicts nightly Airbnb rental prices in New York City using machine learning.  
The workflow includes:

1. **Data loading & preprocessing**  
2. **Feature engineering** (amenity extraction, zipcode aggregates, imputation)  
3. **Modeling using three algorithms:**
   - Random Forest  
   - Elastic Net  
   - XGBoost  
4. **Model evaluation**  
5. **Model selection**  
6. **Diagnostic visualizations**

The final model can be used to generate price predictions for new, unseen listings.



```{r set-up}
#| include: false
#if (!dir.exists("models")) dir.create("models")
```

## 1. Load Libraries & Set global options

```{r packages}

options(repos = c(CRAN = "https://cloud.r-project.org"))
options(download.file.method = "libcurl")

# Install guards (optional). Comment these out if your env already has them.
# core_pkgs <- c("rlang","cli","vctrs","pillar","lifecycle","openssl")
# try(suppressWarnings(install.packages(core_pkgs, type = "binary")), silent = TRUE)

need <- c("tidyverse","janitor","caret","ranger","glmnet","xgboost","rsample","gt","patchwork","scales")
to_get <- need[!sapply(need, requireNamespace, quietly = TRUE)]
if (length(to_get)) install.packages(to_get, type = "binary")

suppressPackageStartupMessages({
  library(tidyverse)
  library(janitor)
  library(caret)
  library(ranger)
  library(glmnet)
  library(xgboost)
  library(rsample)
  library(gt)
  library(patchwork)
  library(scales)
})

set.seed(5656)

# ---- Global plot aesthetics ----
theme_set(theme_minimal(base_size = 12))
update_geom_defaults("point", list(alpha = 0.7))

fmt_dollar <- scales::label_dollar(accuracy = 1, prefix = "$", big.mark = ",", largest_with_cents = 100)
```

## 2. Data Cleaning & Feature Engineering

Data cleaning summary:

- Missing beds are predicted using a regression model based on accommodates  
- Zipcodes with missing values are imputed to the modal zipcode  
- Amenities are separated and counted using tokenization  
- Review ratings are normalized to a 0–5 scale  
- Neighborhood price averages and medians are added without data leakage  

```{r load-data}

analysisData <- suppressWarnings( 
  readr::read_csv( 
    "data/AnalysisData.csv",
    na = c("", "NA", "N/A"),
    col_types = cols( 
      id = col_integer(),
      price = col_double(), 
      beds = col_double(), 
      bedrooms = col_double(), 
      bathrooms = col_double(), 
      accommodates = col_integer(), 
      review_scores_rating = col_double(), 
      number_of_reviews = col_integer(), 
      property_type = col_character(), 
      bed_type = col_character(), 
      neighbourhood_group_cleansed = col_character(), 
      room_type = col_character(), 
      cancellation_policy = col_character(), 
      host_is_superhost = col_character(), 
      instant_bookable = col_character(), 
      zipcode = col_character(), 
      city = col_character(), 
      amenities = col_character(),
      license = col_character(),
      jurisdiction_names = col_character()
      )
                                     ) 
  ) 


scoringData <- suppressWarnings(
  readr::read_csv(
    "data/scoringData.csv",
    na = c("", "NA", "N/A"),
    col_types = cols(
      id = col_integer(),
      beds = col_double(),
      bedrooms = col_double(),
      bathrooms = col_double(),
      accommodates = col_integer(),
      review_scores_rating = col_double(),
      number_of_reviews = col_integer(),
      property_type = col_character(),
      bed_type = col_character(),
      neighbourhood_group_cleansed = col_character(),
      room_type = col_character(),
      cancellation_policy = col_character(),
      host_is_superhost = col_character(),
      instant_bookable = col_character(),
      zipcode = col_character(),
      city = col_character(),
      amenities = col_character(),
      license = col_character(),
      jurisdiction_names = col_character()
    )
  )
)


# checking the col names for those cols and made the characters. 
# 32 = zipcode, 80 = license and 81 = jurisdiction_names
# colnames(analysisData_raw) %>% tibble::enframe(name = "col_number", value = "name")


```



```{r split-data}
# 70/30 split stratified by price
split_obj <- rsample::initial_split(analysisData, prop = 0.70, strata = price)
train <- rsample::training(split_obj)  %>% dplyr::mutate(train_test_score = "train")
test  <- rsample::testing(split_obj)   %>% dplyr::mutate(train_test_score = "test")
if (!is.null(scoringData)) scoringData <- scoringData %>% dplyr::mutate(train_test_score = "score")

# Combine once; engineer once
baseData <- dplyr::bind_rows(train, test, scoringData) %>% janitor::clean_names()

```


```{r feature_engineering}
#| warning: false

# Likely categoricals -> character (we'll factor later)
cat_vars <- c(
  "bed_type","property_type","neighbourhood_group_cleansed","room_type",
  "cancellation_policy","host_is_superhost","instant_bookable",
  "zipcode","city","license","jurisdiction_names"
)
for (v in intersect(cat_vars, names(baseData))) baseData[[v]] <- as.character(baseData[[v]])

# 1) Beds imputation via accommodates (simple linear model)
if (all(c("beds","accommodates") %in% names(baseData))) {
  fit_df <- baseData %>% dplyr::filter(!is.na(beds), !is.na(accommodates))
  if (nrow(fit_df) > 10) {
    lm_beds <- lm(beds ~ accommodates, data = fit_df)
    baseData <- baseData %>%
      dplyr::mutate(
        beds = dplyr::if_else(
          is.na(beds) & !is.na(accommodates),
          round(predict(lm_beds, newdata = dplyr::pick(everything())), 0),
          beds
        )
      )
  }
  if (any(is.na(baseData$beds))) {
    baseData <- baseData %>% dplyr::mutate(beds = dplyr::if_else(is.na(beds), stats::median(beds, na.rm = TRUE), beds))
  }
}

# 2) Zipcode impute to mode
if ("zipcode" %in% names(baseData)) {
  zip_mode <- baseData %>%
    dplyr::filter(!is.na(zipcode)) %>%
    dplyr::count(zipcode, sort = TRUE) %>%
    dplyr::slice_head(n = 1) %>%
    dplyr::pull(zipcode)
  baseData <- baseData %>% dplyr::mutate(zipcode = tidyr::replace_na(zipcode, zip_mode))
}

# 3) Amenities count
if (all(c("id","amenities") %in% names(baseData))) {
  amen_counts <- baseData %>%
    dplyr::select(id, amenities) %>%
    dplyr::mutate(amenities = tidyr::replace_na(amenities, "")) %>%
    tidyr::separate_rows(amenities, sep = ",") %>%
    dplyr::mutate(amenities = stringr::str_trim(amenities)) %>%
    dplyr::filter(amenities != "") %>%
    dplyr::count(id, name = "count_amenities")
  baseData <- baseData %>%
    dplyr::left_join(amen_counts, by = "id") %>%
    dplyr::mutate(count_amenities = tidyr::replace_na(count_amenities, 0L))
}

# 4) Normalized review score (0–5 from 0–100)
baseData <- baseData %>%
  dplyr::mutate(review_score_5 = if ("review_scores_rating" %in% names(.)) review_scores_rating/20 else NA_real_)

# Re-split to keep row roles
train <- baseData %>% dplyr::filter(train_test_score == "train")
test  <- baseData %>% dplyr::filter(train_test_score == "test")
score <- baseData %>% dplyr::filter(train_test_score == "score") %>% { if (nrow(.) == 0) NULL else . }

# 5) Zipcode aggregates (TRAIN ONLY) to avoid leakage
if (all(c("zipcode","price") %in% names(train))) {
  zip_aggs <- train %>%
    dplyr::group_by(zipcode) %>%
    dplyr::summarise(
      zip_price_median = stats::median(price, na.rm = TRUE),
      zip_price_mean   = mean(price, na.rm = TRUE),
      .groups = "drop"
    )
  add_zip <- function(df) {
    out <- df %>% dplyr::left_join(zip_aggs, by = "zipcode")
    out %>%
      dplyr::mutate(
        zip_price_median = tidyr::replace_na(zip_price_median, stats::median(train$price, na.rm = TRUE)),
        zip_price_mean   = tidyr::replace_na(zip_price_mean,   mean(train$price, na.rm = TRUE))
      )
  }
  train <- add_zip(train)
  test  <- add_zip(test)
  if (!is.null(score)) score <- add_zip(score)
}

```



## 3. Exploratory Data Analysis (EDA)
Price distributions show heavy right skew, which motivates log-transformation during modeling. Room type and capacity also show clear price separation.

```{r message=FALSE, warning=FALSE}
p1 <- ggplot(train, aes(price)) +
geom_histogram(bins = 50) +
scale_x_continuous(labels = fmt_dollar) +
labs(title = "Price distribution", x = "Price (USD)", y = "Count")

p2 <- ggplot(train, aes(log1p(price))) +
geom_histogram(bins = 50) +
labs(title = "Log-price distribution", x = "log1p(Price)", y = "Count")

p3 <- train %>%
filter(!is.na(room_type)) %>%
ggplot(aes(x = room_type, y = price)) +
geom_boxplot(outlier.alpha = 0.2) +
scale_y_continuous(labels = fmt_dollar) +
labs(title = "Price by room type", x = "", y = "Price (USD)")

p4 <- ggplot(train, aes(x = accommodates, y = price)) +
geom_point(size = 0.9) +
geom_smooth(se = FALSE) +
scale_y_continuous(labels = fmt_dollar) +
labs(title = "Price vs. accommodates", x = "Accommodates", y = "Price (USD)")

(p1 | p2) / (p3 | p4)


```
## 4. Modeling

Models are trained on **log(price)** to stabilize variance and penalize large errors.
Categorical variables are one-hot encoded and numeric variables are median-imputed.

Models trained:

| Model | Library | Notes |
|-------|---------|-------|
| Random Forest | ranger | Non-linear, robust, interpretable feature importance |
| Elastic Net | glmnet | Linear, with L1/L2 regularization |
| XGBoost | xgboost | Gradient boosting, strong performance on tabular data |


```{r modeling}
# ---- Target on log scale ----

train <- train %>% mutate(price_log = log1p(price))
test  <- test  %>% mutate(price_log = log1p(price))

# Predictors (keep those present)

predictors <- c("host_is_superhost","beds","count_amenities","neighbourhood_group_cleansed","room_type",
"accommodates","bathrooms","bedrooms","cancellation_policy","review_score_5",
"number_of_reviews","property_type","instant_bookable",
"zip_price_median","zip_price_mean","city","zipcode")
predictors <- intersect(predictors, names(train))

# Numeric vs categorical

is_num <- vapply(train[, predictors, drop = FALSE], is.numeric, logical(1))
num_vars <- names(is_num[is_num])
cat_vars_model <- setdiff(predictors, num_vars)

# Numeric: median-impute using TRAIN

num_medians <- purrr::map_dbl(num_vars, ~ median(train[[.x]], na.rm = TRUE))
for (v in num_vars) {
if (anyNA(train[[v]])) train[[v]][is.na(train[[v]])] <- num_medians[[v]]
if (anyNA(test[[v]]))  test[[v]][is.na(test[[v]])]  <- num_medians[[v]]
}

# Categorical: NA -> "Unknown"; rare (<1%) -> "Other" based on TRAIN

pool_rare <- function(x_train, x_new = NULL, thresh = 0.01) {
tx <- ifelse(is.na(x_train), "Unknown", as.character(x_train))
props <- prop.table(table(tx))
keep <- names(props[props >= thresh])
map_levels <- function(z) {
z <- ifelse(is.na(z), "Unknown", as.character(z))
z[!(z %in% keep) & z != "Unknown"] <- "Other"
factor(z, levels = unique(c(keep, "Other", "Unknown")))
}
list(train = map_levels(tx), new = if (!is.null(x_new)) map_levels(x_new) else NULL)
}
for (v in cat_vars_model) {
res <- pool_rare(train[[v]], test[[v]], thresh = 0.01)
train[[v]] <- res$train; test[[v]] <- res$new
}
for (v in cat_vars_model) {
train[[v]] <- as.factor(train[[v]])
test[[v]]  <- factor(test[[v]], levels = levels(train[[v]]))
}

# Model matrices (one-hot)

df_train <- train %>% dplyr::select(all_of(c("price_log", predictors)))
df_test  <- test  %>% dplyr::select(all_of(c("price_log", predictors)))

X_train <- model.matrix(price_log ~ . - 1, data = df_train)
X_test  <- model.matrix(price_log ~ . - 1, data = df_test)

# Align columns

missing_in_test <- setdiff(colnames(X_train), colnames(X_test))
if (length(missing_in_test)) {
X_test <- cbind(X_test, matrix(0, nrow = nrow(X_test), ncol = length(missing_in_test),
dimnames = list(NULL, missing_in_test)))
}
extra_in_test <- setdiff(colnames(X_test), colnames(X_train))
if (length(extra_in_test)) {
X_test <- X_test[, setdiff(colnames(X_test), extra_in_test), drop = FALSE]
}
X_test <- X_test[, colnames(X_train), drop = FALSE]

# Targets

y_train <- df_train$price_log
y_test  <- df_test$price_log

# CV control (used only if you retrain later)

cv_ctrl <- caret::trainControl(method = "repeatedcv", number = 5, repeats = 2, verboseIter = FALSE, allowParallel = TRUE)

```

### Random Forest 
```{r model-one}
# ### ---------------- Random Forest (caret + ranger, saved model) ----------------
# if (!dir.exists("models")) dir.create("models")
# model_path_rf <- "models/rf_fit.rds"
# 
# if (file.exists(model_path_rf)) {
#   message("Loading saved Random Forest model...")
#   rf_fit <- readRDS(model_path_rf)
# } else {
#   message("Training Random Forest model (first time)...")
#   
#   p <- ncol(X_train)
#   rf_grid <- expand.grid(
#     mtry          = unique(pmax(2, floor(c(sqrt(p), sqrt(p)*1.5, sqrt(p)*2)))),
#     splitrule     = c("variance", "extratrees"),
#     min.node.size = c(3, 5, 10)
#   )
# 
#   set.seed(5656)
#   rf_fit <- caret::train(
#     x = X_train,
#     y = y_train,
#     method      = "ranger",
#     trControl   = cv_ctrl,
#     tuneGrid    = rf_grid,
#     num.trees   = 1000,
#     importance  = "impurity",
#     metric      = "RMSE"
#   )
#   
#   saveRDS(rf_fit, model_path_rf)
#   message("Saved Random Forest model to ", model_path_rf)
# }
# 
# # ---- Evaluate on test set ----
# rf_pred_log   <- predict(model_path_rf, newdata = X_test)
# rf_pred_price <- pmax(expm1(rf_pred_log), 0)
# 
# # yardstick metrics if available; else caret::postResample
# if (requireNamespace("yardstick", quietly = TRUE)) {
#   rf_metrics <- tibble::tibble(price = expm1(y_test), .pred = rf_pred_price) |>
#     yardstick::metrics(truth = price, estimate = .pred) |>
#     dplyr::filter(.metric %in% c("rmse","rsq"))
# } else {
#   pm <- caret::postResample(pred = rf_pred_price, obs = expm1(y_test))
#   rf_metrics <- data.frame(.metric = c("rmse","rsq"),
#                            .estimate = c(unname(pm["RMSE"]), unname(pm["Rsquared"])))
# }
# rf_metrics

```

### Elastic Net

```{r model-two}
# ### ---------------- Elastic Net (caret + glmnet, saved model) ----------------
# if (!dir.exists("models")) dir.create("models")
# model_path_en <- "models/en_fit.rds"
# 
# if (file.exists(model_path_en)) {
#   message("Loading saved Elastic Net model...")
#   en_fit <- readRDS(model_path_en)
# } else {
#   message("Training Elastic Net model (first time)...")
#   
#   # Tune across a broad but sensible grid
#   en_grid <- expand.grid(
#     alpha  = seq(0, 1, by = 0.25),                    # ridge (0) -> lasso (1)
#     lambda = 10 ^ seq(2, -4, length.out = 50)         # regularization strength
#   )
#   
#   set.seed(5656)
#   en_fit <- caret::train(
#     x = X_train,
#     y = y_train,
#     method      = "glmnet",
#     trControl   = cv_ctrl,        # 5x2 repeated CV from your prep
#     tuneGrid    = en_grid,
#     standardize = TRUE            # glmnet standardization (recommended)
#     # preProcess = c("center","scale")  # optional, but glmnet standardizes internally
#   )
#   
#   saveRDS(en_fit, model_path_en)
#   message("Saved Elastic Net model to ", model_path_en)
# }
# 
# # ---- Evaluate on test set ----
# en_pred_log   <- predict(en_fit, newdata = X_test)
# en_pred_price <- pmax(expm1(en_pred_log), 0)
# 
# # yardstick metrics if available; else caret::postResample
# if (requireNamespace("yardstick", quietly = TRUE)) {
#   en_metrics <- tibble::tibble(price = expm1(y_test), .pred = en_pred_price) |>
#     yardstick::metrics(truth = price, estimate = .pred) |>
#     dplyr::filter(.metric %in% c("rmse","rsq"))
# } else {
#   pm <- caret::postResample(pred = en_pred_price, obs = expm1(y_test))
#   en_metrics <- data.frame(.metric = c("rmse","rsq"),
#                            .estimate = c(unname(pm["RMSE"]), unname(pm["Rsquared"])))
# }
# en_metrics

# ---- (Optional) Top coefficients for portfolio plot ----
# Extract coefficients at best alpha/lambda
# best_a <- en_fit$bestTune$alpha
# best_l <- en_fit$bestTune$lambda
# coefs  <- as.matrix(coef(en_fit$finalModel, s = best_l))
# coef_tbl <- tibble::tibble(
#   term = rownames(coefs),
#   estimate = as.numeric(coefs)
# ) |>
#   dplyr::filter(term != "(Intercept)") |>
#   dplyr::arrange(dplyr::desc(abs(estimate)))
# 


```

### XGBoost

```{r model-three}

# ### ---------------- XGBoost (caret + xgboost, saved model) ----------------
# if (!dir.exists("models")) dir.create("models")
# model_path_xgb <- "models/xgb_fit.rds"
# 
# if (file.exists(model_path_xgb)) {
#   message("Loading saved XGBoost model...")
#   xgb_fit <- readRDS(model_path_xgb)
# } else {
#   message("Training XGBoost model (first time)...")
#   
#   # caret::xgbTree tuning grid (balanced for speed/quality)
#   xgb_grid <- expand.grid(
#     nrounds = c(400, 800, 1200),     # boosting iterations
#     max_depth = c(4, 6, 8),          # tree depth
#     eta = c(0.03, 0.1),              # learning rate
#     gamma = c(0, 1),                 # minimum loss reduction
#     colsample_bytree = c(0.6, 0.8),  # column subsample
#     min_child_weight = c(1, 3),      # min sum hessian in child
#     subsample = c(0.7, 1.0)          # row subsample
#   )
# 
#   set.seed(5656)
#   xgb_fit <- caret::train(
#     x = X_train,
#     y = y_train,
#     method    = "xgbTree",
#     trControl = cv_ctrl,        # your 5x2 repeated CV
#     tuneGrid  = xgb_grid,
#     metric    = "RMSE",
#     verbose   = FALSE
#   )
#   
#   saveRDS(xgb_fit, model_path_xgb)
#   message("Saved XGBoost model to ", model_path_xgb)
# }
# 
# # ---- Evaluate on test set ---- 
# xgb_pred_log   <- predict(xgb_fit, newdata = X_test)
# xgb_pred_price <- pmax(expm1(xgb_pred_log), 0)
# 
# # yardstick if available; else use caret::postResample
# if (requireNamespace("yardstick", quietly = TRUE)) {
#   xgb_metrics <- tibble::tibble(price = expm1(y_test), .pred = xgb_pred_price) |>
#     yardstick::metrics(truth = price, estimate = .pred) |>
#     dplyr::filter(.metric %in% c("rmse","rsq"))
# } else {
#   pm <- caret::postResample(pred = xgb_pred_price, obs = expm1(y_test))
#   xgb_metrics <- data.frame(.metric = c("rmse","rsq"),
#                             .estimate = c(unname(pm["RMSE"]), unname(pm["Rsquared"])))
# }
# xgb_metrics
# 
# # ---- (Optional) Feature importance (top 20) ----
# # Uses the underlying xgboost booster inside the caret model
# if (requireNamespace("xgboost", quietly = TRUE)) {
#   imp <- xgboost::xgb.importance(feature_names = colnames(X_train),
#                                  model = xgb_fit$finalModel)
# }

```

#### Load Saved Models 
```{r load-saved-models, message=FALSE}
# ---- Load saved models from models/ ----

model_path_rf  <- "models/rf_fit.rds"
model_path_en  <- "models/en_fit.rds"
model_path_xgb <- "models/xgb_fit.rds"

rf_fit  <- if (file.exists(model_path_rf))  readRDS(model_path_rf)  else NULL
en_fit  <- if (file.exists(model_path_en))  readRDS(model_path_en)  else NULL
xgb_fit <- if (file.exists(model_path_xgb)) readRDS(model_path_xgb) else NULL

# Helpers

bt <- function(x) pmax(expm1(x), 0)  # back-transform from log1p
eval_model <- function(fit, X_new, y_true_log) {
stopifnot(!is.null(fit))
pred_log <- predict(fit, newdata = X_new)
pred     <- bt(as.numeric(pred_log))
obs      <- bt(as.numeric(y_true_log))
preds_df <- tibble::tibble(price = obs, .pred = pred, resid = obs - pred)

if (requireNamespace("yardstick", quietly = TRUE)) {
mets <- preds_df |>
yardstick::metrics(truth = price, estimate = .pred) |>
dplyr::filter(.metric %in% c("rmse","rsq"))
} else {
pm <- caret::postResample(pred = preds_df$.pred, obs = preds_df$price)
mets <- tibble::tibble(.metric = c("rmse","rsq"),
.estimate = c(unname(pm["RMSE"]), unname(pm["Rsquared"])))
}
list(preds = preds_df, metrics = mets)
}


```




## 5. Model Comparison & Selection

```{r compare-models, message=FALSE}


bt <- function(x) pmax(expm1(x), 0)

# Align a data.frame version of X to the model's required predictor names
align_newdata <- function(fit, X) {
  df <- as.data.frame(X)
  # Try to get the predictor names the model expects
  req <- NULL
  # ranger via caret
  if (!is.null(fit$finalModel) && inherits(fit$finalModel, "ranger")) {
    req <- try(fit$finalModel$forest$independent.variable.names, silent = TRUE)
  }
  # generic caret fallback (uses trainingData columns, minus .outcome)
  if (is.null(req) || inherits(req, "try-error")) {
    if (!is.null(fit$trainingData)) {
      req <- setdiff(colnames(fit$trainingData), ".outcome")
    }
  }
  # Final fallback: just use colnames(X) (shouldn't happen, but keeps robustness)
  if (is.null(req)) req <- colnames(df)

  # Add missing as zeros, drop extras, order to req
  miss <- setdiff(req, colnames(df))
  if (length(miss)) for (m in miss) df[[m]] <- 0
  extra <- setdiff(colnames(df), req)
  if (length(extra)) df <- df[, setdiff(colnames(df), extra), drop = FALSE]
  df[, req, drop = FALSE]
}

# Robust evaluator: try aligned X first; then raw test df as fallback
eval_model <- function(fit, X_test, y_test, test_df) {
  pred_log <- NULL

  nd <- try(align_newdata(fit, X_test), silent = TRUE)
  if (!inherits(nd, "try-error")) {
    pred_log <- try(predict(fit, newdata = nd), silent = TRUE)
  }
  if (inherits(pred_log, "try-error") || is.null(pred_log)) {
    pred_log <- try(predict(fit, newdata = test_df), silent = TRUE)
  }
  if (inherits(pred_log, "try-error") || is.null(pred_log)) return(NULL)

  preds <- tibble::tibble(price = expm1(y_test), .pred = bt(pred_log))
  list(
    preds   = preds,
    metrics = preds |>
      yardstick::metrics(truth = price, estimate = .pred) |>
      dplyr::filter(.metric %in% c("rmse","rsq"))
  )
}

# ---- Evaluate whatever models are available ----
results <- list()
if (exists("rf_fit")  && !is.null(rf_fit))  results$RF  <- eval_model(rf_fit,  X_test, y_test, test)
if (exists("en_fit")  && !is.null(en_fit))  results$EN  <- eval_model(en_fit,  X_test, y_test, test)
if (exists("xgb_fit") && !is.null(xgb_fit)) results$XGB <- eval_model(xgb_fit, X_test, y_test, test)

# Keep only successful evaluations
results <- purrr::compact(results)
stopifnot(length(results) > 0)

# ---- Build comparison table ----
metrics_tbl <- purrr::imap_dfr(results, ~ .x$metrics %>% dplyr::mutate(model = .y)) %>%
  dplyr::select(model, .metric, .estimate) %>%
  dplyr::mutate(.metric = tolower(.metric)) %>%
  tidyr::pivot_wider(names_from = .metric, values_from = .estimate) %>%
  dplyr::mutate(
    rmse = as.numeric(rmse),
    rsq  = as.numeric(rsq)
  ) %>%
  dplyr::arrange(rmse)

# Optional pretty table
metrics_tbl %>%
  gt::gt() %>%
  gt::tab_header(title = "Model Performance on Test Set") %>%
  gt::cols_label(rmse = "RMSE ($)", rsq = "R²") %>%
  gt::tab_style(style = gt::cell_text(weight = "bold"),
                locations = gt::cells_column_labels())

# ---- Pick best & prep for visuals ----
best_name <- metrics_tbl$model[1]
best_preds <- results[[best_name]]$preds %>%
  dplyr::transmute(price, pred = .pred, resid = price - .pred)

message(sprintf("✅ Best model: %s — RMSE $%.1f | R² %.3f",
                best_name, metrics_tbl$rmse[1], metrics_tbl$rsq[1]))

best_name <- as.character(metrics_tbl$model[1])

# Map the winning label -> actual model object
best_fit <- switch(best_name,
  RF  = rf_fit,
  EN  = en_fit,
  XGB = xgb_fit
)
stopifnot(!is.null(best_fit))

```
The table shows performance on the held-out test set.  
Lower RMSE = better. Higher R² = better.


The best-performing model is **`r best_name`**, with:

- **RMSE:** `r scales::dollar(round(metrics_tbl$rmse[1],1))`
- **R²:** `r round(metrics_tbl$rsq[1], 3)`

This model is used for all diagnostics and predictions.

---

## 6. Model Diagnostics
Interpretation of Diagnostic Plots

Predicted vs Actual:
The XGBoost model closely tracks the 45° reference line, indicating good predictive accuracy. Most points cluster in the lower–mid price range, with some under-prediction at higher prices — a common pattern when extreme nightly rates are rare in the training data.

Residuals vs Predicted:
Residuals are centered around zero with no strong patterns, suggesting the model is not systematically over- or under-predicting across most price ranges. A few large positive and negative residuals indicate outliers (listings with unusually high or low prices).

Residuals Q–Q Plot:
The residual distribution deviates from perfect normality in the tails — again driven by extreme listings — but the majority fall near the reference line, supporting that model errors are approximately symmetric and well-behaved.

Overall, the diagnostics show that the XGBoost model generalizes well, captures the main pricing structure in the data, and only struggles on a small number of extreme cases (luxury or atypical listings).
```{r diagnostics, message=FALSE, warning=FALSE, fig.width=11, fig.height=7}

# --- prerequisites ---
if (!exists("metrics_tbl")) stop("metrics_tbl not found.")
if (!exists("results"))     stop("results not found.")
if (!requireNamespace("hexbin", quietly = TRUE)) install.packages("hexbin")
if (!requireNamespace("patchwork", quietly = TRUE)) install.packages("patchwork")
library(patchwork)

# pick best model (lowercase metric names)
best_row  <- metrics_tbl %>% dplyr::slice_min(rmse, n = 1)
best_name <- as.character(best_row$model)

# (re)build best_preds if needed
if (!exists("best_preds") || is.null(best_preds)) {
  if (!is.null(results[[best_name]]) && !is.null(results[[best_name]]$preds)) {
    best_preds <- results[[best_name]]$preds %>%
      dplyr::transmute(price, pred = .pred, resid = price - .pred)
  } else {
    if (!exists("bt")) bt <- function(x) pmax(expm1(x), 0)
    pred_log <- predict(switch(best_name, RF=rf_fit, EN=en_fit, XGB=xgb_fit), newdata = X_test)
    best_preds <- tibble::tibble(
      price = bt(as.numeric(y_test)),
      pred  = bt(as.numeric(pred_log))
    ) %>% dplyr::mutate(resid = price - pred)
  }
}

# helpers
if (!exists("fmt_dollar")) fmt_dollar <- scales::label_dollar(accuracy = 1, prefix = "$", big.mark = ",")

# set symmetric residual limits (trim extreme outliers for readability)
r_lim <- stats::quantile(abs(best_preds$resid), 0.99, na.rm = TRUE) |> as.numeric()

# --- Plot A: Predicted vs Actual (bigger at top) ---
pA <- ggplot(best_preds, aes(x = price, y = pred)) +
  geom_hex(bins = 35, show.legend = TRUE) +
  scale_fill_viridis_c(name = "Count") +
  geom_abline(slope = 1, intercept = 0, linetype = 2, linewidth = 0.7) +
  coord_equal() +
  scale_x_continuous(labels = fmt_dollar, breaks = scales::breaks_pretty(6)) +
  scale_y_continuous(labels = fmt_dollar, breaks = scales::breaks_pretty(6)) +
  labs(title = sprintf("%s — Predicted vs Actual", best_name),
       x = "Actual price", y = "Predicted price") +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(face = "bold", size = 16),
    legend.position = "right"
  )

# --- Plot B: Residuals vs Predicted ---
pB <- ggplot(best_preds, aes(x = pred, y = resid)) +
  geom_point(alpha = 0.25, size = 0.8) +
  geom_smooth(method = "loess", se = FALSE, linewidth = 0.9) +
  geom_hline(yintercept = 0, linetype = 2, linewidth = 0.6) +
  scale_x_continuous(labels = fmt_dollar, breaks = scales::breaks_pretty(5)) +
  scale_y_continuous(limits = c(-r_lim, r_lim)) +
  labs(title = "Residuals vs Predicted",
       x = "Predicted price", y = "Residual") +
  theme_minimal(base_size = 12) +
  theme(plot.title = element_text(face = "bold"))

# --- Plot C: Residuals Q–Q Plot ---
pC <- ggplot(best_preds, aes(sample = resid)) +
  stat_qq(alpha = 0.5, size = 1) +
  stat_qq_line(color = "red", linewidth = 0.8) +
  labs(title = "Residuals — Q–Q Plot",
       x = "Theoretical Quantiles", y = "Sample Quantiles") +
  theme_minimal(base_size = 12) +
  theme(plot.title = element_text(face = "bold"))
# Add subtle margins so labels never collide with the panel edge
pA <- pA + theme(plot.margin = margin(6, 12, 6, 6))
pB <- pB + theme(plot.margin = margin(6, 12, 6, 6))
pC <- pC + theme(plot.margin = margin(6, 6, 6, 12))

# Build a spacer to force separation
sp <- patchwork::plot_spacer()

# Bottom row with a spacer column in the middle
bottom_row <- (pB | sp | pC) + patchwork::plot_layout(widths = c(1, 0.08, 1))

# Final layout: large top, separated bottoms
final_diag <- (pA) / bottom_row +
  patchwork::plot_layout(heights = c(1.35, 1), guides = "collect") &
  theme(legend.position = "right")

final_diag


```



## 7. Feature Importance

```{r importance-or-coefs, fig.width=7, fig.height=4}
# Ensure best_fit exists even if this chunk is run alone
if (!exists("best_fit") || is.null(best_fit)) {
  best_fit <- switch(best_name, RF = rf_fit, EN = en_fit, XGB = xgb_fit)
  stopifnot(!is.null(best_fit))
}

if (best_name == "RF") {

# caret::varImp works on the trained ranger model wrapped by caret

vi <- caret::varImp(best_fit)$importance %>%
tibble::rownames_to_column("feature") %>%
dplyr::arrange(dplyr::desc(Overall)) %>%
dplyr::slice(1:20)
ggplot(vi, aes(x = reorder(feature, Overall), y = Overall)) +
geom_col(fill = "#4E79A7") + coord_flip() +
labs(title = "Top 20 Features — Random Forest", x = "", y = "Importance")

} else if (best_name == "XGB") {

# Use underlying xgboost booster inside caret model

if (requireNamespace("xgboost", quietly = TRUE)) {
imp <- xgboost::xgb.importance(
feature_names = colnames(X_train),
model = best_fit$finalModel
)
imp <- imp %>% dplyr::slice(1:20)
ggplot(imp, aes(x = reorder(Feature, Gain), y = Gain)) +
geom_col(fill = "#E67E22") + coord_flip() +
labs(title = "Top 20 Features — XGBoost", x = "", y = "Gain")
}
} else if (best_name == "EN") {

# Coefficients from glmnet at best lambda

best_l <- best_fit$bestTune$lambda
cf <- as.matrix(coef(best_fit$finalModel, s = best_l))
en_coefs <- tibble::tibble(term = rownames(cf), estimate = as.numeric(cf)) %>%
dplyr::filter(term != "(Intercept)") %>%
dplyr::arrange(dplyr::desc(abs(estimate))) %>%
dplyr::slice(1:20)
ggplot(en_coefs, aes(x = reorder(term, estimate), y = estimate)) +
geom_col(fill = "#2E86C1") + coord_flip() +
labs(title = "Top 20 Elastic Net Coefficients", x = "", y = "Coefficient (log-price scale)")
}

```

::: callout-note
**Model-specific interpretation notes**

- **XGBoost “Gain”**
  - Gain tells us how important a feature is by measuring how much it helps the model reduce prediction error.
  - Higher gain = the feature contributed more to improving accuracy.

- **Elastic Net coefficients**
  - Coefficients are learned on the log-price scale, which means they translate to percentage changes in price.
  - To interpret a coefficient: convert using `exp(coef) - 1`.
  - Example: a coefficient of **+0.07** ≈ **+7.25%** higher price, because `exp(0.07) - 1 ≈ 0.0725`.
:::


```{r prep-score-matrix, include=FALSE}
# Only run if score exists
if (!is.null(score)) {
  stopifnot(exists("predictors"))

  # Pool rare categories in SCORE using TRAIN as reference
  if (exists("cat_vars_model")) {
    for (v in intersect(cat_vars_model, names(train))) {
      tx <- ifelse(is.na(train[[v]]), "Unknown", as.character(train[[v]]))
      props <- prop.table(table(tx))
      keep  <- names(props[props >= 0.01])
      map_levels <- function(z) {
        z <- ifelse(is.na(z), "Unknown", as.character(z))
        z[!(z %in% keep) & z != "Unknown"] <- "Other"
        factor(z, levels = unique(c(keep, "Other", "Unknown")))
      }
      if (v %in% names(score)) score[[v]] <- map_levels(score[[v]])
    }
    for (v in intersect(cat_vars_model, names(score))) {
      score[[v]] <- factor(score[[v]], levels = levels(train[[v]]))
    }
  }

  # Numeric imputation with TRAIN medians
  if (exists("num_vars")) {
    for (v in intersect(num_vars, names(score))) {
      if (anyNA(score[[v]])) {
        med <- stats::median(train[[v]], na.rm = TRUE)
        score[[v]][is.na(score[[v]])] <- med
      }
    }
  }

  # Build model matrix for SCORE and align to X_train
  df_score <- score %>% dplyr::select(all_of(predictors))
  X_score  <- model.matrix(~ . - 1, data = df_score)

  miss <- setdiff(colnames(X_train), colnames(X_score))
  if (length(miss)) {
    X_score <- cbind(
      X_score,
      matrix(0, nrow = nrow(X_score), ncol = length(miss), dimnames = list(NULL, miss))
    )
  }
  extra <- setdiff(colnames(X_score), colnames(X_train))
  if (length(extra)) X_score <- X_score[, setdiff(colnames(X_score), extra), drop = FALSE]
  X_score <- X_score[, colnames(X_train), drop = FALSE]
}
```

## 8. Predicting on New (Unseen) Data

After training and selecting the best model, predictions were generated for `scoringData.csv`, which contains new Airbnb listings with no recorded prices. The goal is to estimate their nightly rental cost for submission.

The training dataset reflects real NYC Airbnb market behavior. To understand market structure, I summarized and visualized the price distribution in the analysis dataset. The median nightly price is approximately **$110**, and most listings fall between **$50–$250** per night, with a long right-tail driven by premium and luxury properties.

After producing model predictions for the unseen listings, I compared their distribution to the observed market data.  
The predicted density curve aligns closely with the historical market distribution, which indicates:

- the model generalizes well,
- predictions fall within realistic market ranges, and
- the model does not systematically over- or under-estimate prices.

This provides confidence that the model outputs plausible nightly price estimates for new listings.

```{r predict-new-data}
stopifnot(exists("best_fit") && !is.null(best_fit))
if (is.null(score)) stop("No scoring data found at 'data/scoringData.csv'.")

# Helper: get the exact feature names the trained model expects
get_req_names <- function(fit) {
  # xgboost via caret stores names in finalModel$feature_names
  nm <- try(fit$finalModel$feature_names, silent = TRUE)
  if (!inherits(nm, "try-error") && !is.null(nm)) return(as.character(nm))
  # generic caret fallback: columns used during training (minus .outcome)
  if (!is.null(fit$trainingData)) {
    return(setdiff(colnames(fit$trainingData), ".outcome"))
  }
  # last resort: use X_train colnames if present
  if (exists("X_train")) return(colnames(X_train))
  stop("Could not infer required feature names for prediction.")
}

# 1) Ensure we have a score matrix built (from prep-score-matrix)
stopifnot(exists("X_score"))

# 2) Align X_score to the model’s required names
req <- get_req_names(best_fit)

# Start from current X_score; add missing zeros, drop extras, order columns
X_sc <- X_score
miss <- setdiff(req, colnames(X_sc))
if (length(miss)) {
  X_sc <- cbind(
    X_sc,
    matrix(0, nrow = nrow(X_sc), ncol = length(miss), dimnames = list(NULL, miss))
  )
}
extra <- setdiff(colnames(X_sc), req)
if (length(extra)) X_sc <- X_sc[, setdiff(colnames(X_sc), extra), drop = FALSE]
X_sc <- X_sc[, req, drop = FALSE]

# Ensure it is a numeric matrix for xgboost
X_sc <- as.matrix(X_sc)
storage.mode(X_sc) <- "double"

# 3) Predict (matrix path first; fallback to data.frame if needed)
pred_log <- try(predict(best_fit, newdata = X_sc), silent = TRUE)
if (inherits(pred_log, "try-error") || is.null(pred_log)) {
  pred_log <- predict(best_fit, newdata = as.data.frame(X_sc))
}

# 4) Back-transform from log1p and clip at zero
pred_price <- pmax(expm1(as.numeric(pred_log)), 0)

# 5) Build submission file: ONLY id and price
submission <- score %>% dplyr::select(id) %>% dplyr::mutate(price = pred_price)

# Sanity checks
stopifnot(all(c("id","price") %in% names(submission)))
stopifnot(!any(is.na(submission$price)))
stopifnot(!any(submission$price < 0))

# 6) Write CSV per competition spec
readr::write_csv(submission, "data/submission.csv")

# Preview
# head(submission)


```
```{r}
library(ggplot2)
ggplot(analysisData, aes(price)) +
  geom_histogram(bins = 60, fill = "pink", alpha = 0.6) +
  scale_x_continuous(labels = scales::dollar) +
  labs(title = "Distribution of Nightly Prices (Observed Market)")

```

```{r}
stopifnot(exists("submission"), exists("train"))
preds_df <- submission %>% dplyr::mutate(source = "Predicted (Scoring)")
train_df <- train %>% dplyr::transmute(price, source = "Observed (Train)")

ggplot(dplyr::bind_rows(
        preds_df %>% dplyr::rename(price = price),
        train_df),
       aes(x = price, fill = source)) +
  geom_density(alpha = 0.4) +
  scale_x_continuous(labels = fmt_dollar) +
  labs(title = "Where do predicted prices sit vs. the training market?",
       x = "Nightly price", y = "Density", fill = "") +
  theme_minimal()
```
```{r include=FALSE}
stopifnot(exists("best_preds"))

calib <- best_preds %>%
  dplyr::mutate(q = cut(price,
                        breaks = quantile(price, probs = seq(0,1,0.1), na.rm=TRUE),
                        include.lowest = TRUE)) %>%
  dplyr::group_by(q) %>%
  dplyr::summarise(actual = median(price, na.rm=TRUE),
                   pred   = median(pred,  na.rm=TRUE),
                   .groups = "drop")

ggplot(calib, aes(x = actual, y = pred)) +
  geom_abline(linetype = 2) +
  geom_point(size = 2) +
  scale_x_continuous(labels = fmt_dollar) +
  scale_y_continuous(labels = fmt_dollar) +
  labs(title = "Quantile calibration on held-out test set",
       x = "Median actual (by quantile)", y = "Median predicted") +
  theme_minimal()
```

```{r include=FALSE}
stopifnot(exists("submission"), exists("score"))

score_pred <- score %>%
  dplyr::select(id, room_type, accommodates, neighbourhood_group_cleansed) %>%
  dplyr::inner_join(submission, by = "id") %>%
  dplyr::mutate(room_type = forcats::fct_lump_n(room_type, n = 4))

pA <- ggplot(score_pred, aes(x = room_type, y = price)) +
  geom_boxplot(outlier.alpha = 0.15) +
  scale_y_continuous(labels = fmt_dollar) +
  labs(title = "Predicted price by room type (scoring set)", x = "", y = "Predicted price")

pB <- ggplot(score_pred, aes(x = accommodates, y = price)) +
  geom_point(alpha = 0.3) +
  geom_smooth(se = FALSE) +
  scale_y_continuous(labels = fmt_dollar) +
  labs(title = "Predicted price vs. accommodates", x = "Accommodates", y = "Predicted price")

pA / pB
```



```{r include=FALSE}
top_k <- submission %>%
  dplyr::arrange(dplyr::desc(price)) %>%
  dplyr::slice(1:15) %>%
  dplyr::left_join(score %>% dplyr::select(id, room_type, accommodates, neighbourhood_group_cleansed),
                   by = "id") %>%
  dplyr::mutate(rank = dplyr::row_number()) %>%
  dplyr::select(rank, id, price, room_type, accommodates, neighbourhood_group_cleansed)

gt::gt(top_k) %>%
  gt::fmt_currency(columns = "price", currency = "USD", decimals = 0) %>%
  gt::tab_header(title = "Top 15 predicted listings in the scoring set") %>%
  gt::cols_label(neighbourhood_group_cleansed = "Neighborhood grp")
```


## 8. Conclusion

This project built a complete machine-learning workflow to predict nightly Airbnb rental prices in New York City — from data cleaning and feature engineering to model training, evaluation, and pricing new listings.

After evaluating model performance on a held-out test set, **XGBoost** achieved the strongest results, with the lowest RMSE and highest R². Random Forest and Elastic Net performed reasonably well, but XGBoost captured non-linear relationships in the pricing structure more effectively.

**Key takeaways:**

- Major pricing drivers included **accommodates**, **room type**, **amenities**, and **zipcode-level price aggregates**, all of which align with real-world market behavior.
- Residual diagnostics showed errors centered around zero with minimal skew, indicating that the model is not systematically over- or under-pricing listings.
- Feature importance confirmed economic intuition: larger spaces, premium neighborhoods, and richer amenities consistently lead to higher nightly rates.
- Model predictions closely matched actual market prices, with only a small number of high-end luxury listings producing wider residuals.

### Scoring New Data

A cleaned version of the scoring dataset was passed through the trained pipeline, including:

- missing-value handling  
- categorical alignment  
- amenities engineering  
- zipcode-level pricing features  
- log-to-price back-transformation  

Final predictions were exported in the required Kaggle format (`id`, `price`) and saved as `submission.csv`.  
The distribution of predicted prices closely overlapped the historical market data, suggesting realistic, market-consistent pricing.

### Potential Next Improvements

**Enhancements possible with the current dataset:**
- Engineer richer host reputation features (e.g., review volume tiers, normalized ratings, superhost indicators)
- Build a two-stage modeling approach that first identifies luxury listings, then predicts price within each segment
- Add additional zipcode-level market signals (e.g., median neighborhood price, price density)

**Enhancements requiring additional data:**
- Incorporate calendar effects such as weekends, holidays, or seasonal demand
- Add spatial features such as distance to landmarks, subway access, or walkability
- Evaluate more advanced ensemble methods (LightGBM, CatBoost, or stacked models)

### Final Note

With a strong predictive model and a fully reproducible scoring pipeline, this approach can now price **new Airbnb listings** — valuable for hosts optimizing revenue, investors evaluating short-term rental markets, or platforms designing dynamic pricing tools.
