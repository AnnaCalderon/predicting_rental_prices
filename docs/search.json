[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Predicting Airbnb Rental Prices in NYC",
    "section": "",
    "text": "This project predicts nightly Airbnb rental prices in New York City using machine learning.\nThe workflow includes:\n✅ Data loading & preprocessing\n✅ Feature engineering (amenity extraction, zipcode aggregates, imputation)\n✅ Modeling using three algorithms: - Random Forest\n- Elastic Net\n- XGBoost\n✅ Model evaluation\n✅ Model selection\n✅ Diagnostic visualizations\nThe final model can be used to generate price predictions for new, unseen listings."
  },
  {
    "objectID": "about.html#overview",
    "href": "about.html#overview",
    "title": "Predicting Airbnb Rental Prices in NYC",
    "section": "",
    "text": "This project predicts nightly Airbnb rental prices in New York City using machine learning.\nThe workflow includes:\n✅ Data loading & preprocessing\n✅ Feature engineering (amenity extraction, zipcode aggregates, imputation)\n✅ Modeling using three algorithms: - Random Forest\n- Elastic Net\n- XGBoost\n✅ Model evaluation\n✅ Model selection\n✅ Diagnostic visualizations\nThe final model can be used to generate price predictions for new, unseen listings."
  },
  {
    "objectID": "about.html#load-libraries-set-global-options",
    "href": "about.html#load-libraries-set-global-options",
    "title": "Predicting Airbnb Rental Prices in NYC",
    "section": "1. Load Libraries & Set global options",
    "text": "1. Load Libraries & Set global options\n\n\nCode\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\noptions(download.file.method = \"libcurl\")\n\n# Install guards (optional). Comment these out if your env already has them.\n# core_pkgs &lt;- c(\"rlang\",\"cli\",\"vctrs\",\"pillar\",\"lifecycle\",\"openssl\")\n# try(suppressWarnings(install.packages(core_pkgs, type = \"binary\")), silent = TRUE)\n\nneed &lt;- c(\"tidyverse\",\"janitor\",\"caret\",\"ranger\",\"glmnet\",\"xgboost\",\"rsample\",\"gt\",\"patchwork\",\"scales\")\nto_get &lt;- need[!sapply(need, requireNamespace, quietly = TRUE)]\nif (length(to_get)) install.packages(to_get, type = \"binary\")\n\nsuppressPackageStartupMessages({\n  library(tidyverse)\n  library(janitor)\n  library(caret)\n  library(ranger)\n  library(glmnet)\n  library(xgboost)\n  library(rsample)\n  library(gt)\n  library(patchwork)\n  library(scales)\n})\n\nset.seed(5656)\n\n# ---- Global plot aesthetics ----\ntheme_set(theme_minimal(base_size = 12))\nupdate_geom_defaults(\"point\", list(alpha = 0.7))\n\nfmt_dollar &lt;- scales::label_dollar(accuracy = 1, prefix = \"$\", big.mark = \",\", largest_with_cents = 100)"
  },
  {
    "objectID": "about.html#data-cleaning-feature-engineering",
    "href": "about.html#data-cleaning-feature-engineering",
    "title": "Predicting Airbnb Rental Prices in NYC",
    "section": "2. Data Cleaning & Feature Engineering",
    "text": "2. Data Cleaning & Feature Engineering\nData cleaning summary:\n\nMissing beds are predicted using a regression model based on accommodates\n\nZipcodes with missing values are imputed to the modal zipcode\n\nAmenities are separated and counted using tokenization\n\nReview ratings are normalized to a 0–5 scale\n\nNeighborhood price averages and medians are added without data leakage\n\n\n\nCode\nanalysisData &lt;- suppressWarnings(\n  readr::read_csv(\n    \"data/analysis_data.csv\",\n    col_types = cols(\n      id = col_integer(),\n      price = col_double(),\n      beds = col_double(),\n      bedrooms = col_double(),\n      bathrooms = col_double(),\n      accommodates = col_integer(),\n      review_scores_rating = col_double(),\n      number_of_reviews = col_integer(),\n      property_type = col_character(),\n      bed_type = col_character(),\n      neighbourhood_group_cleansed = col_character(),\n      room_type = col_character(),\n      cancellation_policy = col_character(),\n      host_is_superhost = col_character(),\n      instant_bookable  = col_character(),\n      zipcode = col_character(),\n      city = col_character(),\n      amenities = col_character()\n    )\n  )\n)\n\n\n\nscoring_exists &lt;- file.exists(\"scoringData.csv\")\nscoringData &lt;- if (scoring_exists) readr::read_csv(\"scoringData.csv\", show_col_types = FALSE) else NULL\n\n# Ensure common types BEFORE combining (zipcode as character)\n\nif (\"zipcode\" %in% names(analysisData)) analysisData &lt;- analysisData %&gt;% mutate(zipcode = as.character(zipcode))\nif (!is.null(scoringData) && \"zipcode\" %in% names(scoringData)) scoringData &lt;- scoringData %&gt;% mutate(zipcode = as.character(zipcode))\n\n\n\n\nCode\n# 70/30 split stratified by price\n\nsplit_obj &lt;- rsample::initial_split(analysisData, prop = 0.70, strata = price)\ntrain &lt;- rsample::training(split_obj)  %&gt;% mutate(train_test_score = \"train\")\ntest  &lt;- rsample::testing(split_obj)   %&gt;% mutate(train_test_score = \"test\")\nif (!is.null(scoringData)) scoringData &lt;- scoringData %&gt;% mutate(train_test_score = \"score\")\n\n# Combine once; engineer once\n\nbaseData &lt;- bind_rows(train, test, scoringData) %&gt;% clean_names()\n\n\n\n\nCode\n# Likely categoricals -&gt; character (we'll factor later)\n\ncat_vars &lt;- c(\"bed_type\",\"property_type\",\"neighbourhood_group_cleansed\",\"room_type\",\n\"cancellation_policy\",\"host_is_superhost\",\"instant_bookable\",\"zipcode\",\"city\")\nfor (v in intersect(cat_vars, names(baseData))) baseData[[v]] &lt;- as.character(baseData[[v]])\n\n# 1) Beds imputation via accommodates (simple linear model)\n\nif (all(c(\"beds\",\"accommodates\") %in% names(baseData))) {\nfit_df &lt;- baseData %&gt;% filter(!is.na(beds), !is.na(accommodates))\nif (nrow(fit_df) &gt; 10) {\nlm_beds &lt;- lm(beds ~ accommodates, data = fit_df)\nbaseData &lt;- baseData %&gt;%\nmutate(\n  beds = if_else(\n    is.na(beds) & !is.na(accommodates),\n    round(predict(lm_beds, newdata = pick(everything())), 0),\n    beds\n  )\n)\n\n}\nif (any(is.na(baseData$beds))) {\nbaseData &lt;- baseData %&gt;% mutate(beds = if_else(is.na(beds), median(beds, na.rm = TRUE), beds))\n}\n}\n\n# 2) Zipcode impute to mode  ✅ FIXED\nif (\"zipcode\" %in% names(baseData)) {\n  zip_mode &lt;- baseData %&gt;%\n    dplyr::filter(!is.na(zipcode)) %&gt;%\n    dplyr::count(zipcode, sort = TRUE) %&gt;%\n    dplyr::slice_head(n = 1) %&gt;%        # &lt;= avoids the xgboost::slice conflict\n    dplyr::pull(zipcode)\n\n  baseData &lt;- baseData %&gt;% dplyr::mutate(zipcode = tidyr::replace_na(zipcode, zip_mode))\n}\n\n\n# 3) Amenities count\n\nif (all(c(\"id\",\"amenities\") %in% names(baseData))) {\namen_counts &lt;- baseData %&gt;%\nselect(id, amenities) %&gt;%\nmutate(amenities = replace_na(amenities, \"\")) %&gt;%\ntidyr::separate_rows(amenities, sep = \",\") %&gt;%\nmutate(amenities = stringr::str_trim(amenities)) %&gt;%\nfilter(amenities != \"\") %&gt;%\ncount(id, name = \"count_amenities\")\nbaseData &lt;- baseData %&gt;%\nleft_join(amen_counts, by = \"id\") %&gt;%\nmutate(count_amenities = replace_na(count_amenities, 0L))\n}\n\n# 4) Normalized review score (0–5 from 0–100)\n\nbaseData &lt;- baseData %&gt;% mutate(review_score_5 = if (\"review_scores_rating\" %in% names(.)) review_scores_rating/20 else NA_real_)\n\n# Re-split to keep row roles\n\ntrain &lt;- baseData %&gt;% filter(train_test_score == \"train\")\ntest  &lt;- baseData %&gt;% filter(train_test_score == \"test\")\nscore &lt;- baseData %&gt;% filter(train_test_score == \"score\") %&gt;% { if (nrow(.) == 0) NULL else . }\n\n# Zipcode aggregates (TRAIN ONLY) to avoid leakage\n\nif (all(c(\"zipcode\",\"price\") %in% names(train))) {\nzip_aggs &lt;- train %&gt;%\ngroup_by(zipcode) %&gt;%\nsummarise(zip_price_median = median(price, na.rm = TRUE),\nzip_price_mean   = mean(price,   na.rm = TRUE),\n.groups = \"drop\")\nadd_zip &lt;- function(df) {\nout &lt;- df %&gt;% left_join(zip_aggs, by = \"zipcode\")\nout %&gt;%\nmutate(zip_price_median = replace_na(zip_price_median, median(train$price, na.rm = TRUE)),\nzip_price_mean   = replace_na(zip_price_mean,   mean(train$price, na.rm = TRUE)))\n}\ntrain &lt;- add_zip(train); test &lt;- add_zip(test); if (!is.null(score)) score &lt;- add_zip(score)\n}"
  },
  {
    "objectID": "about.html#exploratory-data-analysis-eda",
    "href": "about.html#exploratory-data-analysis-eda",
    "title": "Predicting Airbnb Rental Prices in NYC",
    "section": "3. Exploratory Data Analysis (EDA)",
    "text": "3. Exploratory Data Analysis (EDA)\nPrice distributions show heavy right skew, which motivates log-transformation during modeling. Room type and capacity also show clear price separation.\n\n\nCode\np1 &lt;- ggplot(train, aes(price)) +\ngeom_histogram(bins = 50) +\nscale_x_continuous(labels = fmt_dollar) +\nlabs(title = \"Price distribution\", x = \"Price (USD)\", y = \"Count\")\n\np2 &lt;- ggplot(train, aes(log1p(price))) +\ngeom_histogram(bins = 50) +\nlabs(title = \"Log-price distribution\", x = \"log1p(Price)\", y = \"Count\")\n\np3 &lt;- train %&gt;%\nfilter(!is.na(room_type)) %&gt;%\nggplot(aes(x = room_type, y = price)) +\ngeom_boxplot(outlier.alpha = 0.2) +\nscale_y_continuous(labels = fmt_dollar) +\nlabs(title = \"Price by room type\", x = \"\", y = \"Price (USD)\")\n\np4 &lt;- ggplot(train, aes(x = accommodates, y = price)) +\ngeom_point(size = 0.9) +\ngeom_smooth(se = FALSE) +\nscale_y_continuous(labels = fmt_dollar) +\nlabs(title = \"Price vs. accommodates\", x = \"Accommodates\", y = \"Price (USD)\")\n\n(p1 | p2) / (p3 | p4)"
  },
  {
    "objectID": "about.html#modeling",
    "href": "about.html#modeling",
    "title": "Predicting Airbnb Rental Prices in NYC",
    "section": "4. Modeling",
    "text": "4. Modeling\nModels are trained on log(price) to stabilize variance and penalize large errors. Categorical variables are one-hot encoded and numeric variables are median-imputed.\nModels trained:\n\n\n\n\n\n\n\n\nModel\nLibrary\nNotes\n\n\n\n\nRandom Forest\nranger\nNon-linear, robust, interpretable feature importance\n\n\nElastic Net\nglmnet\nLinear, with L1/L2 regularization\n\n\nXGBoost\nxgboost\nGradient boosting, strong performance on tabular data\n\n\n\n\n\nCode\n# ---- Target on log scale ----\n\ntrain &lt;- train %&gt;% mutate(price_log = log1p(price))\ntest  &lt;- test  %&gt;% mutate(price_log = log1p(price))\n\n# Predictors (keep those present)\n\npredictors &lt;- c(\"host_is_superhost\",\"beds\",\"count_amenities\",\"neighbourhood_group_cleansed\",\"room_type\",\n\"accommodates\",\"bathrooms\",\"bedrooms\",\"cancellation_policy\",\"review_score_5\",\n\"number_of_reviews\",\"property_type\",\"instant_bookable\",\n\"zip_price_median\",\"zip_price_mean\",\"city\",\"zipcode\")\npredictors &lt;- intersect(predictors, names(train))\n\n# Numeric vs categorical\n\nis_num &lt;- vapply(train[, predictors, drop = FALSE], is.numeric, logical(1))\nnum_vars &lt;- names(is_num[is_num])\ncat_vars_model &lt;- setdiff(predictors, num_vars)\n\n# Numeric: median-impute using TRAIN\n\nnum_medians &lt;- purrr::map_dbl(num_vars, ~ median(train[[.x]], na.rm = TRUE))\nfor (v in num_vars) {\nif (anyNA(train[[v]])) train[[v]][is.na(train[[v]])] &lt;- num_medians[[v]]\nif (anyNA(test[[v]]))  test[[v]][is.na(test[[v]])]  &lt;- num_medians[[v]]\n}\n\n# Categorical: NA -&gt; \"Unknown\"; rare (&lt;1%) -&gt; \"Other\" based on TRAIN\n\npool_rare &lt;- function(x_train, x_new = NULL, thresh = 0.01) {\ntx &lt;- ifelse(is.na(x_train), \"Unknown\", as.character(x_train))\nprops &lt;- prop.table(table(tx))\nkeep &lt;- names(props[props &gt;= thresh])\nmap_levels &lt;- function(z) {\nz &lt;- ifelse(is.na(z), \"Unknown\", as.character(z))\nz[!(z %in% keep) & z != \"Unknown\"] &lt;- \"Other\"\nfactor(z, levels = unique(c(keep, \"Other\", \"Unknown\")))\n}\nlist(train = map_levels(tx), new = if (!is.null(x_new)) map_levels(x_new) else NULL)\n}\nfor (v in cat_vars_model) {\nres &lt;- pool_rare(train[[v]], test[[v]], thresh = 0.01)\ntrain[[v]] &lt;- res$train; test[[v]] &lt;- res$new\n}\nfor (v in cat_vars_model) {\ntrain[[v]] &lt;- as.factor(train[[v]])\ntest[[v]]  &lt;- factor(test[[v]], levels = levels(train[[v]]))\n}\n\n# Model matrices (one-hot)\n\ndf_train &lt;- train %&gt;% dplyr::select(all_of(c(\"price_log\", predictors)))\ndf_test  &lt;- test  %&gt;% dplyr::select(all_of(c(\"price_log\", predictors)))\n\nX_train &lt;- model.matrix(price_log ~ . - 1, data = df_train)\nX_test  &lt;- model.matrix(price_log ~ . - 1, data = df_test)\n\n# Align columns\n\nmissing_in_test &lt;- setdiff(colnames(X_train), colnames(X_test))\nif (length(missing_in_test)) {\nX_test &lt;- cbind(X_test, matrix(0, nrow = nrow(X_test), ncol = length(missing_in_test),\ndimnames = list(NULL, missing_in_test)))\n}\nextra_in_test &lt;- setdiff(colnames(X_test), colnames(X_train))\nif (length(extra_in_test)) {\nX_test &lt;- X_test[, setdiff(colnames(X_test), extra_in_test), drop = FALSE]\n}\nX_test &lt;- X_test[, colnames(X_train), drop = FALSE]\n\n# Targets\n\ny_train &lt;- df_train$price_log\ny_test  &lt;- df_test$price_log\n\n# CV control (used only if you retrain later)\n\ncv_ctrl &lt;- caret::trainControl(method = \"repeatedcv\", number = 5, repeats = 2, verboseIter = FALSE, allowParallel = TRUE)\n\n\n\n\nCode\n# ### ---------------- Random Forest (caret + ranger, saved model) ----------------\n# if (!dir.exists(\"models\")) dir.create(\"models\")\n# model_path_rf &lt;- \"models/rf_fit.rds\"\n# \n# if (file.exists(model_path_rf)) {\n#   message(\"Loading saved Random Forest model...\")\n#   rf_fit &lt;- readRDS(model_path_rf)\n# } else {\n#   message(\"Training Random Forest model (first time)...\")\n#   \n#   p &lt;- ncol(X_train)\n#   rf_grid &lt;- expand.grid(\n#     mtry          = unique(pmax(2, floor(c(sqrt(p), sqrt(p)*1.5, sqrt(p)*2)))),\n#     splitrule     = c(\"variance\", \"extratrees\"),\n#     min.node.size = c(3, 5, 10)\n#   )\n# \n#   set.seed(5656)\n#   rf_fit &lt;- caret::train(\n#     x = X_train,\n#     y = y_train,\n#     method      = \"ranger\",\n#     trControl   = cv_ctrl,\n#     tuneGrid    = rf_grid,\n#     num.trees   = 1000,\n#     importance  = \"impurity\",\n#     metric      = \"RMSE\"\n#   )\n#   \n#   saveRDS(rf_fit, model_path_rf)\n#   message(\"Saved Random Forest model to \", model_path_rf)\n# }\n# \n# # ---- Evaluate on test set ----\n# rf_pred_log   &lt;- predict(model_path_rf, newdata = X_test)\n# rf_pred_price &lt;- pmax(expm1(rf_pred_log), 0)\n# \n# # yardstick metrics if available; else caret::postResample\n# if (requireNamespace(\"yardstick\", quietly = TRUE)) {\n#   rf_metrics &lt;- tibble::tibble(price = expm1(y_test), .pred = rf_pred_price) |&gt;\n#     yardstick::metrics(truth = price, estimate = .pred) |&gt;\n#     dplyr::filter(.metric %in% c(\"rmse\",\"rsq\"))\n# } else {\n#   pm &lt;- caret::postResample(pred = rf_pred_price, obs = expm1(y_test))\n#   rf_metrics &lt;- data.frame(.metric = c(\"rmse\",\"rsq\"),\n#                            .estimate = c(unname(pm[\"RMSE\"]), unname(pm[\"Rsquared\"])))\n# }\n# rf_metrics\n\n\n\n\nCode\n# ### ---------------- Elastic Net (caret + glmnet, saved model) ----------------\n# if (!dir.exists(\"models\")) dir.create(\"models\")\n# model_path_en &lt;- \"models/en_fit.rds\"\n# \n# if (file.exists(model_path_en)) {\n#   message(\"Loading saved Elastic Net model...\")\n#   en_fit &lt;- readRDS(model_path_en)\n# } else {\n#   message(\"Training Elastic Net model (first time)...\")\n#   \n#   # Tune across a broad but sensible grid\n#   en_grid &lt;- expand.grid(\n#     alpha  = seq(0, 1, by = 0.25),                    # ridge (0) -&gt; lasso (1)\n#     lambda = 10 ^ seq(2, -4, length.out = 50)         # regularization strength\n#   )\n#   \n#   set.seed(5656)\n#   en_fit &lt;- caret::train(\n#     x = X_train,\n#     y = y_train,\n#     method      = \"glmnet\",\n#     trControl   = cv_ctrl,        # 5x2 repeated CV from your prep\n#     tuneGrid    = en_grid,\n#     standardize = TRUE            # glmnet standardization (recommended)\n#     # preProcess = c(\"center\",\"scale\")  # optional, but glmnet standardizes internally\n#   )\n#   \n#   saveRDS(en_fit, model_path_en)\n#   message(\"Saved Elastic Net model to \", model_path_en)\n# }\n# \n# # ---- Evaluate on test set ----\n# en_pred_log   &lt;- predict(en_fit, newdata = X_test)\n# en_pred_price &lt;- pmax(expm1(en_pred_log), 0)\n# \n# # yardstick metrics if available; else caret::postResample\n# if (requireNamespace(\"yardstick\", quietly = TRUE)) {\n#   en_metrics &lt;- tibble::tibble(price = expm1(y_test), .pred = en_pred_price) |&gt;\n#     yardstick::metrics(truth = price, estimate = .pred) |&gt;\n#     dplyr::filter(.metric %in% c(\"rmse\",\"rsq\"))\n# } else {\n#   pm &lt;- caret::postResample(pred = en_pred_price, obs = expm1(y_test))\n#   en_metrics &lt;- data.frame(.metric = c(\"rmse\",\"rsq\"),\n#                            .estimate = c(unname(pm[\"RMSE\"]), unname(pm[\"Rsquared\"])))\n# }\n# en_metrics\n\n# ---- (Optional) Top coefficients for portfolio plot ----\n# Extract coefficients at best alpha/lambda\n# best_a &lt;- en_fit$bestTune$alpha\n# best_l &lt;- en_fit$bestTune$lambda\n# coefs  &lt;- as.matrix(coef(en_fit$finalModel, s = best_l))\n# coef_tbl &lt;- tibble::tibble(\n#   term = rownames(coefs),\n#   estimate = as.numeric(coefs)\n# ) |&gt;\n#   dplyr::filter(term != \"(Intercept)\") |&gt;\n#   dplyr::arrange(dplyr::desc(abs(estimate)))\n# \n\n\n\n\nCode\n# ### ---------------- XGBoost (caret + xgboost, saved model) ----------------\n# if (!dir.exists(\"models\")) dir.create(\"models\")\n# model_path_xgb &lt;- \"models/xgb_fit.rds\"\n# \n# if (file.exists(model_path_xgb)) {\n#   message(\"Loading saved XGBoost model...\")\n#   xgb_fit &lt;- readRDS(model_path_xgb)\n# } else {\n#   message(\"Training XGBoost model (first time)...\")\n#   \n#   # caret::xgbTree tuning grid (balanced for speed/quality)\n#   xgb_grid &lt;- expand.grid(\n#     nrounds = c(400, 800, 1200),     # boosting iterations\n#     max_depth = c(4, 6, 8),          # tree depth\n#     eta = c(0.03, 0.1),              # learning rate\n#     gamma = c(0, 1),                 # minimum loss reduction\n#     colsample_bytree = c(0.6, 0.8),  # column subsample\n#     min_child_weight = c(1, 3),      # min sum hessian in child\n#     subsample = c(0.7, 1.0)          # row subsample\n#   )\n# \n#   set.seed(5656)\n#   xgb_fit &lt;- caret::train(\n#     x = X_train,\n#     y = y_train,\n#     method    = \"xgbTree\",\n#     trControl = cv_ctrl,        # your 5x2 repeated CV\n#     tuneGrid  = xgb_grid,\n#     metric    = \"RMSE\",\n#     verbose   = FALSE\n#   )\n#   \n#   saveRDS(xgb_fit, model_path_xgb)\n#   message(\"Saved XGBoost model to \", model_path_xgb)\n# }\n# \n# # ---- Evaluate on test set ---- \n# xgb_pred_log   &lt;- predict(xgb_fit, newdata = X_test)\n# xgb_pred_price &lt;- pmax(expm1(xgb_pred_log), 0)\n# \n# # yardstick if available; else use caret::postResample\n# if (requireNamespace(\"yardstick\", quietly = TRUE)) {\n#   xgb_metrics &lt;- tibble::tibble(price = expm1(y_test), .pred = xgb_pred_price) |&gt;\n#     yardstick::metrics(truth = price, estimate = .pred) |&gt;\n#     dplyr::filter(.metric %in% c(\"rmse\",\"rsq\"))\n# } else {\n#   pm &lt;- caret::postResample(pred = xgb_pred_price, obs = expm1(y_test))\n#   xgb_metrics &lt;- data.frame(.metric = c(\"rmse\",\"rsq\"),\n#                             .estimate = c(unname(pm[\"RMSE\"]), unname(pm[\"Rsquared\"])))\n# }\n# xgb_metrics\n# \n# # ---- (Optional) Feature importance (top 20) ----\n# # Uses the underlying xgboost booster inside the caret model\n# if (requireNamespace(\"xgboost\", quietly = TRUE)) {\n#   imp &lt;- xgboost::xgb.importance(feature_names = colnames(X_train),\n#                                  model = xgb_fit$finalModel)\n# }\n\n\n\n\nCode\n# ---- Load saved models from models/ ----\n\nmodel_path_rf  &lt;- \"models/rf_fit.rds\"\nmodel_path_en  &lt;- \"models/en_fit.rds\"\nmodel_path_xgb &lt;- \"models/xgb_fit.rds\"\n\nrf_fit  &lt;- if (file.exists(model_path_rf))  readRDS(model_path_rf)  else NULL\nen_fit  &lt;- if (file.exists(model_path_en))  readRDS(model_path_en)  else NULL\nxgb_fit &lt;- if (file.exists(model_path_xgb)) readRDS(model_path_xgb) else NULL\n\n# Helpers\n\nbt &lt;- function(x) pmax(expm1(x), 0)  # back-transform from log1p\neval_model &lt;- function(fit, X_new, y_true_log) {\nstopifnot(!is.null(fit))\npred_log &lt;- predict(fit, newdata = X_new)\npred     &lt;- bt(as.numeric(pred_log))\nobs      &lt;- bt(as.numeric(y_true_log))\npreds_df &lt;- tibble::tibble(price = obs, .pred = pred, resid = obs - pred)\n\nif (requireNamespace(\"yardstick\", quietly = TRUE)) {\nmets &lt;- preds_df |&gt;\nyardstick::metrics(truth = price, estimate = .pred) |&gt;\ndplyr::filter(.metric %in% c(\"rmse\",\"rsq\"))\n} else {\npm &lt;- caret::postResample(pred = preds_df$.pred, obs = preds_df$price)\nmets &lt;- tibble::tibble(.metric = c(\"rmse\",\"rsq\"),\n.estimate = c(unname(pm[\"RMSE\"]), unname(pm[\"Rsquared\"])))\n}\nlist(preds = preds_df, metrics = mets)\n}"
  },
  {
    "objectID": "about.html#model-comparison-selection",
    "href": "about.html#model-comparison-selection",
    "title": "Predicting Airbnb Rental Prices in NYC",
    "section": "5. Model Comparison & Selection",
    "text": "5. Model Comparison & Selection\n\n\nCode\nbt &lt;- function(x) pmax(expm1(x), 0)\n\n# Align a data.frame version of X to the model's required predictor names\nalign_newdata &lt;- function(fit, X) {\n  df &lt;- as.data.frame(X)\n  # Try to get the predictor names the model expects\n  req &lt;- NULL\n  # ranger via caret\n  if (!is.null(fit$finalModel) && inherits(fit$finalModel, \"ranger\")) {\n    req &lt;- try(fit$finalModel$forest$independent.variable.names, silent = TRUE)\n  }\n  # generic caret fallback (uses trainingData columns, minus .outcome)\n  if (is.null(req) || inherits(req, \"try-error\")) {\n    if (!is.null(fit$trainingData)) {\n      req &lt;- setdiff(colnames(fit$trainingData), \".outcome\")\n    }\n  }\n  # Final fallback: just use colnames(X) (shouldn't happen, but keeps robustness)\n  if (is.null(req)) req &lt;- colnames(df)\n\n  # Add missing as zeros, drop extras, order to req\n  miss &lt;- setdiff(req, colnames(df))\n  if (length(miss)) for (m in miss) df[[m]] &lt;- 0\n  extra &lt;- setdiff(colnames(df), req)\n  if (length(extra)) df &lt;- df[, setdiff(colnames(df), extra), drop = FALSE]\n  df[, req, drop = FALSE]\n}\n\n# Robust evaluator: try aligned X first; then raw test df as fallback\neval_model &lt;- function(fit, X_test, y_test, test_df) {\n  pred_log &lt;- NULL\n\n  nd &lt;- try(align_newdata(fit, X_test), silent = TRUE)\n  if (!inherits(nd, \"try-error\")) {\n    pred_log &lt;- try(predict(fit, newdata = nd), silent = TRUE)\n  }\n  if (inherits(pred_log, \"try-error\") || is.null(pred_log)) {\n    pred_log &lt;- try(predict(fit, newdata = test_df), silent = TRUE)\n  }\n  if (inherits(pred_log, \"try-error\") || is.null(pred_log)) return(NULL)\n\n  preds &lt;- tibble::tibble(price = expm1(y_test), .pred = bt(pred_log))\n  list(\n    preds   = preds,\n    metrics = preds |&gt;\n      yardstick::metrics(truth = price, estimate = .pred) |&gt;\n      dplyr::filter(.metric %in% c(\"rmse\",\"rsq\"))\n  )\n}\n\n# ---- Evaluate whatever models are available ----\nresults &lt;- list()\nif (exists(\"rf_fit\")  && !is.null(rf_fit))  results$RF  &lt;- eval_model(rf_fit,  X_test, y_test, test)\nif (exists(\"en_fit\")  && !is.null(en_fit))  results$EN  &lt;- eval_model(en_fit,  X_test, y_test, test)\nif (exists(\"xgb_fit\") && !is.null(xgb_fit)) results$XGB &lt;- eval_model(xgb_fit, X_test, y_test, test)\n\n# Keep only successful evaluations\nresults &lt;- purrr::compact(results)\nstopifnot(length(results) &gt; 0)\n\n# ---- Build comparison table ----\nmetrics_tbl &lt;- purrr::imap_dfr(results, ~ .x$metrics %&gt;% dplyr::mutate(model = .y)) %&gt;%\n  dplyr::select(model, .metric, .estimate) %&gt;%\n  dplyr::mutate(.metric = tolower(.metric)) %&gt;%\n  tidyr::pivot_wider(names_from = .metric, values_from = .estimate) %&gt;%\n  dplyr::mutate(\n    rmse = as.numeric(rmse),\n    rsq  = as.numeric(rsq)\n  ) %&gt;%\n  dplyr::arrange(rmse)\n\n# Optional pretty table\nmetrics_tbl %&gt;%\n  gt::gt() %&gt;%\n  gt::tab_header(title = \"Model Performance on Test Set\") %&gt;%\n  gt::cols_label(rmse = \"RMSE ($)\", rsq = \"R²\") %&gt;%\n  gt::tab_style(style = gt::cell_text(weight = \"bold\"),\n                locations = gt::cells_column_labels())\n\n\n\n\n\n\n\n\nModel Performance on Test Set\n\n\nmodel\nRMSE ($)\nR²\n\n\n\n\nXGB\n65.65469\n0.6414969\n\n\nRF\n66.96682\n0.6360707\n\n\nEN\n70.17521\n0.5860496\n\n\n\n\n\n\n\nCode\n# ---- Pick best & prep for visuals ----\nbest_name &lt;- metrics_tbl$model[1]\nbest_preds &lt;- results[[best_name]]$preds %&gt;%\n  dplyr::transmute(price, pred = .pred, resid = price - .pred)\n\nmessage(sprintf(\"✅ Best model: %s — RMSE $%.1f | R² %.3f\",\n                best_name, metrics_tbl$rmse[1], metrics_tbl$rsq[1]))\n\nbest_name &lt;- as.character(metrics_tbl$model[1])\n\n# Map the winning label -&gt; actual model object\nbest_fit &lt;- switch(best_name,\n  RF  = rf_fit,\n  EN  = en_fit,\n  XGB = xgb_fit\n)\nstopifnot(!is.null(best_fit))\n\n\nThe table shows performance on the held-out test set.\nLower RMSE = better. Higher R² = better.\nThe best-performing model is XGB, with:\n\nRMSE: $65.70\nR²: 0.641\n\nThis model is used for all diagnostics and predictions."
  },
  {
    "objectID": "about.html#model-diagnostics",
    "href": "about.html#model-diagnostics",
    "title": "Predicting Airbnb Rental Prices in NYC",
    "section": "6. Model Diagnostics",
    "text": "6. Model Diagnostics\nInterpretation of Diagnostic Plots\nPredicted vs Actual: The XGBoost model closely tracks the 45° reference line, indicating good predictive accuracy. Most points cluster in the lower–mid price range, with some under-prediction at higher prices — a common pattern when extreme nightly rates are rare in the training data.\nResiduals vs Predicted: Residuals are centered around zero with no strong patterns, suggesting the model is not systematically over- or under-predicting across most price ranges. A few large positive and negative residuals indicate outliers (listings with unusually high or low prices).\nResiduals Q–Q Plot: The residual distribution deviates from perfect normality in the tails — again driven by extreme listings — but the majority fall near the reference line, supporting that model errors are approximately symmetric and well-behaved.\nOverall, the diagnostics show that the XGBoost model generalizes well, captures the main pricing structure in the data, and only struggles on a small number of extreme cases (luxury or atypical listings).\n\n\nCode\n# --- prerequisites ---\nif (!exists(\"metrics_tbl\")) stop(\"metrics_tbl not found.\")\nif (!exists(\"results\"))     stop(\"results not found.\")\nif (!requireNamespace(\"hexbin\", quietly = TRUE)) install.packages(\"hexbin\")\nif (!requireNamespace(\"patchwork\", quietly = TRUE)) install.packages(\"patchwork\")\nlibrary(patchwork)\n\n# pick best model (lowercase metric names)\nbest_row  &lt;- metrics_tbl %&gt;% dplyr::slice_min(rmse, n = 1)\nbest_name &lt;- as.character(best_row$model)\n\n# (re)build best_preds if needed\nif (!exists(\"best_preds\") || is.null(best_preds)) {\n  if (!is.null(results[[best_name]]) && !is.null(results[[best_name]]$preds)) {\n    best_preds &lt;- results[[best_name]]$preds %&gt;%\n      dplyr::transmute(price, pred = .pred, resid = price - .pred)\n  } else {\n    if (!exists(\"bt\")) bt &lt;- function(x) pmax(expm1(x), 0)\n    pred_log &lt;- predict(switch(best_name, RF=rf_fit, EN=en_fit, XGB=xgb_fit), newdata = X_test)\n    best_preds &lt;- tibble::tibble(\n      price = bt(as.numeric(y_test)),\n      pred  = bt(as.numeric(pred_log))\n    ) %&gt;% dplyr::mutate(resid = price - pred)\n  }\n}\n\n# helpers\nif (!exists(\"fmt_dollar\")) fmt_dollar &lt;- scales::label_dollar(accuracy = 1, prefix = \"$\", big.mark = \",\")\n\n# set symmetric residual limits (trim extreme outliers for readability)\nr_lim &lt;- stats::quantile(abs(best_preds$resid), 0.99, na.rm = TRUE) |&gt; as.numeric()\n\n# --- Plot A: Predicted vs Actual (bigger at top) ---\npA &lt;- ggplot(best_preds, aes(x = price, y = pred)) +\n  geom_hex(bins = 35, show.legend = TRUE) +\n  scale_fill_viridis_c(name = \"Count\") +\n  geom_abline(slope = 1, intercept = 0, linetype = 2, linewidth = 0.7) +\n  coord_equal() +\n  scale_x_continuous(labels = fmt_dollar, breaks = scales::breaks_pretty(6)) +\n  scale_y_continuous(labels = fmt_dollar, breaks = scales::breaks_pretty(6)) +\n  labs(title = sprintf(\"%s — Predicted vs Actual\", best_name),\n       x = \"Actual price\", y = \"Predicted price\") +\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    legend.position = \"right\"\n  )\n\n# --- Plot B: Residuals vs Predicted ---\npB &lt;- ggplot(best_preds, aes(x = pred, y = resid)) +\n  geom_point(alpha = 0.25, size = 0.8) +\n  geom_smooth(method = \"loess\", se = FALSE, linewidth = 0.9) +\n  geom_hline(yintercept = 0, linetype = 2, linewidth = 0.6) +\n  scale_x_continuous(labels = fmt_dollar, breaks = scales::breaks_pretty(5)) +\n  scale_y_continuous(limits = c(-r_lim, r_lim)) +\n  labs(title = \"Residuals vs Predicted\",\n       x = \"Predicted price\", y = \"Residual\") +\n  theme_minimal(base_size = 12) +\n  theme(plot.title = element_text(face = \"bold\"))\n\n# --- Plot C: Residuals Q–Q Plot ---\npC &lt;- ggplot(best_preds, aes(sample = resid)) +\n  stat_qq(alpha = 0.5, size = 1) +\n  stat_qq_line(color = \"red\", linewidth = 0.8) +\n  labs(title = \"Residuals — Q–Q Plot\",\n       x = \"Theoretical Quantiles\", y = \"Sample Quantiles\") +\n  theme_minimal(base_size = 12) +\n  theme(plot.title = element_text(face = \"bold\"))\n# Add subtle margins so labels never collide with the panel edge\npA &lt;- pA + theme(plot.margin = margin(6, 12, 6, 6))\npB &lt;- pB + theme(plot.margin = margin(6, 12, 6, 6))\npC &lt;- pC + theme(plot.margin = margin(6, 6, 6, 12))\n\n# Build a spacer to force separation\nsp &lt;- patchwork::plot_spacer()\n\n# Bottom row with a spacer column in the middle\nbottom_row &lt;- (pB | sp | pC) + patchwork::plot_layout(widths = c(1, 0.08, 1))\n\n# Final layout: large top, separated bottoms\nfinal_diag &lt;- (pA) / bottom_row +\n  patchwork::plot_layout(heights = c(1.35, 1), guides = \"collect\") &\n  theme(legend.position = \"right\")\n\nfinal_diag\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 119 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 119 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "about.html#feature-importance",
    "href": "about.html#feature-importance",
    "title": "Predicting Airbnb Rental Prices in NYC",
    "section": "7. Feature Importance",
    "text": "7. Feature Importance\n\n\nCode\n# Ensure best_fit exists even if this chunk is run alone\nif (!exists(\"best_fit\") || is.null(best_fit)) {\n  best_fit &lt;- switch(best_name, RF = rf_fit, EN = en_fit, XGB = xgb_fit)\n  stopifnot(!is.null(best_fit))\n}\n\nif (best_name == \"RF\") {\n\n# caret::varImp works on the trained ranger model wrapped by caret\n\nvi &lt;- caret::varImp(best_fit)$importance %&gt;%\ntibble::rownames_to_column(\"feature\") %&gt;%\ndplyr::arrange(dplyr::desc(Overall)) %&gt;%\ndplyr::slice(1:20)\nggplot(vi, aes(x = reorder(feature, Overall), y = Overall)) +\ngeom_col(fill = \"#4E79A7\") + coord_flip() +\nlabs(title = \"Top 20 Features — Random Forest\", x = \"\", y = \"Importance\")\n\n} else if (best_name == \"XGB\") {\n\n# Use underlying xgboost booster inside caret model\n\nif (requireNamespace(\"xgboost\", quietly = TRUE)) {\nimp &lt;- xgboost::xgb.importance(\nfeature_names = colnames(X_train),\nmodel = best_fit$finalModel\n)\nimp &lt;- imp %&gt;% dplyr::slice(1:20)\nggplot(imp, aes(x = reorder(Feature, Gain), y = Gain)) +\ngeom_col(fill = \"#E67E22\") + coord_flip() +\nlabs(title = \"Top 20 Features — XGBoost\", x = \"\", y = \"Gain\")\n}\n} else if (best_name == \"EN\") {\n\n# Coefficients from glmnet at best lambda\n\nbest_l &lt;- best_fit$bestTune$lambda\ncf &lt;- as.matrix(coef(best_fit$finalModel, s = best_l))\nen_coefs &lt;- tibble::tibble(term = rownames(cf), estimate = as.numeric(cf)) %&gt;%\ndplyr::filter(term != \"(Intercept)\") %&gt;%\ndplyr::arrange(dplyr::desc(abs(estimate))) %&gt;%\ndplyr::slice(1:20)\nggplot(en_coefs, aes(x = reorder(term, estimate), y = estimate)) +\ngeom_col(fill = \"#2E86C1\") + coord_flip() +\nlabs(title = \"Top 20 Elastic Net Coefficients\", x = \"\", y = \"Coefficient (log-price scale)\")\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nModel-specific interpretation notes\n\nXGBoost “Gain”\n\nGain tells us how important a feature is by measuring how much it helps the model reduce prediction error.\nHigher gain = the feature contributed more to improving accuracy.\n\nElastic Net coefficients\n\nCoefficients are learned on the log-price scale, which means they translate to percentage changes in price.\nTo interpret a coefficient: convert using exp(coef) - 1.\nExample: a coefficient of +0.07 ≈ +7.25% higher price, because exp(0.07) - 1 ≈ 0.0725."
  },
  {
    "objectID": "about.html#conclusion",
    "href": "about.html#conclusion",
    "title": "Predicting Airbnb Rental Prices in NYC",
    "section": "8. Conclusion",
    "text": "8. Conclusion\nThis project successfully built and compared three machine learning models to predict nightly Airbnb prices in New York City.\nAfter evaluating performance on a held-out test set, XGBoost achieved the best results, with the lowest RMSE and highest R², indicating strong predictive accuracy and generalization.\nKey takeaways:\n\nPrices are strongly driven by room type, accommodates capacity, amenities, and neighborhood-level pricing.\nThe model performs well across the majority of listings, with only a small number of outlier properties showing large residuals (typically luxury or highly atypical units).\nDiagnostics confirm that model errors are centered around zero and mostly symmetric, suggesting a stable and unbiased fit.\nFeature importance aligns with intuition: higher capacity, prime neighborhoods, and better amenities are associated with higher nightly rates.\n\n\nWhat could improve future performance?\n\nInclude calendar/seasonality features (weekends, holidays, peak tourism months)\nIncorporate host reputation (e.g., number of past stays, cancellation rates, response time)\nFit a model that accounts for geographic distance to landmarks (Times Square, major subway hubs)\nTry advanced boosting approaches (LightGBM, CatBoost) or stacking models\nModel price as a two-step process:\n\nPredict if a listing is luxury / high-tier\n\nPredict price within each tier\n\n\n\n\nFinal note:\nThe XGBoost model can now be applied to new, unseen listings to provide estimated nightly prices, making it useful for hosts, investors, and platforms evaluating pricing strategies."
  }
]