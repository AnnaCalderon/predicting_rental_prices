[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Predicting Airbnb Rental Prices in NYC",
    "section": "",
    "text": "This project predicts nightly Airbnb rental prices in New York City using machine learning.\nThe workflow includes:\n\nData loading & preprocessing\n\nFeature engineering (amenity extraction, zipcode aggregates, imputation)\n\nModeling using three algorithms:\n\nRandom Forest\n\nElastic Net\n\nXGBoost\n\n\nModel evaluation\n\nModel selection\n\nDiagnostic visualizations\n\nThe final model can be used to generate price predictions for new, unseen listings."
  },
  {
    "objectID": "about.html#overview",
    "href": "about.html#overview",
    "title": "Predicting Airbnb Rental Prices in NYC",
    "section": "",
    "text": "This project predicts nightly Airbnb rental prices in New York City using machine learning.\nThe workflow includes:\n\nData loading & preprocessing\n\nFeature engineering (amenity extraction, zipcode aggregates, imputation)\n\nModeling using three algorithms:\n\nRandom Forest\n\nElastic Net\n\nXGBoost\n\n\nModel evaluation\n\nModel selection\n\nDiagnostic visualizations\n\nThe final model can be used to generate price predictions for new, unseen listings."
  },
  {
    "objectID": "about.html#load-libraries-set-global-options",
    "href": "about.html#load-libraries-set-global-options",
    "title": "Predicting Airbnb Rental Prices in NYC",
    "section": "1. Load Libraries & Set global options",
    "text": "1. Load Libraries & Set global options\n\n\nCode\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\noptions(download.file.method = \"libcurl\")\n\n# Install guards (optional). Comment these out if your env already has them.\n# core_pkgs &lt;- c(\"rlang\",\"cli\",\"vctrs\",\"pillar\",\"lifecycle\",\"openssl\")\n# try(suppressWarnings(install.packages(core_pkgs, type = \"binary\")), silent = TRUE)\n\nneed &lt;- c(\"tidyverse\",\"janitor\",\"caret\",\"ranger\",\"glmnet\",\"xgboost\",\"rsample\",\"gt\",\"patchwork\",\"scales\")\nto_get &lt;- need[!sapply(need, requireNamespace, quietly = TRUE)]\nif (length(to_get)) install.packages(to_get, type = \"binary\")\n\nsuppressPackageStartupMessages({\n  library(tidyverse)\n  library(janitor)\n  library(caret)\n  library(ranger)\n  library(glmnet)\n  library(xgboost)\n  library(rsample)\n  library(gt)\n  library(patchwork)\n  library(scales)\n})\n\nset.seed(5656)\n\n# ---- Global plot aesthetics ----\ntheme_set(theme_minimal(base_size = 12))\nupdate_geom_defaults(\"point\", list(alpha = 0.7))\n\nfmt_dollar &lt;- scales::label_dollar(accuracy = 1, prefix = \"$\", big.mark = \",\", largest_with_cents = 100)"
  },
  {
    "objectID": "about.html#data-cleaning-feature-engineering",
    "href": "about.html#data-cleaning-feature-engineering",
    "title": "Predicting Airbnb Rental Prices in NYC",
    "section": "2. Data Cleaning & Feature Engineering",
    "text": "2. Data Cleaning & Feature Engineering\nData cleaning summary:\n\nMissing beds are predicted using a regression model based on accommodates\n\nZipcodes with missing values are imputed to the modal zipcode\n\nAmenities are separated and counted using tokenization\n\nReview ratings are normalized to a 0–5 scale\n\nNeighborhood price averages and medians are added without data leakage\n\n\n\nCode\nanalysisData &lt;- suppressWarnings( \n  readr::read_csv( \n    \"data/AnalysisData.csv\",\n    na = c(\"\", \"NA\", \"N/A\"),\n    col_types = cols( \n      id = col_integer(),\n      price = col_double(), \n      beds = col_double(), \n      bedrooms = col_double(), \n      bathrooms = col_double(), \n      accommodates = col_integer(), \n      review_scores_rating = col_double(), \n      number_of_reviews = col_integer(), \n      property_type = col_character(), \n      bed_type = col_character(), \n      neighbourhood_group_cleansed = col_character(), \n      room_type = col_character(), \n      cancellation_policy = col_character(), \n      host_is_superhost = col_character(), \n      instant_bookable = col_character(), \n      zipcode = col_character(), \n      city = col_character(), \n      amenities = col_character(),\n      license = col_character(),\n      jurisdiction_names = col_character()\n      )\n                                     ) \n  ) \n\n\nscoringData &lt;- suppressWarnings(\n  readr::read_csv(\n    \"data/scoringData.csv\",\n    na = c(\"\", \"NA\", \"N/A\"),\n    col_types = cols(\n      id = col_integer(),\n      beds = col_double(),\n      bedrooms = col_double(),\n      bathrooms = col_double(),\n      accommodates = col_integer(),\n      review_scores_rating = col_double(),\n      number_of_reviews = col_integer(),\n      property_type = col_character(),\n      bed_type = col_character(),\n      neighbourhood_group_cleansed = col_character(),\n      room_type = col_character(),\n      cancellation_policy = col_character(),\n      host_is_superhost = col_character(),\n      instant_bookable = col_character(),\n      zipcode = col_character(),\n      city = col_character(),\n      amenities = col_character(),\n      license = col_character(),\n      jurisdiction_names = col_character()\n    )\n  )\n)\n\n\n# checking the col names for those cols and made the characters. \n# 32 = zipcode, 80 = license and 81 = jurisdiction_names\n# colnames(analysisData_raw) %&gt;% tibble::enframe(name = \"col_number\", value = \"name\")\n\n\n\n\nCode\n# 70/30 split stratified by price\nsplit_obj &lt;- rsample::initial_split(analysisData, prop = 0.70, strata = price)\ntrain &lt;- rsample::training(split_obj)  %&gt;% dplyr::mutate(train_test_score = \"train\")\ntest  &lt;- rsample::testing(split_obj)   %&gt;% dplyr::mutate(train_test_score = \"test\")\nif (!is.null(scoringData)) scoringData &lt;- scoringData %&gt;% dplyr::mutate(train_test_score = \"score\")\n\n# Combine once; engineer once\nbaseData &lt;- dplyr::bind_rows(train, test, scoringData) %&gt;% janitor::clean_names()\n\n\n\n\nCode\n# Likely categoricals -&gt; character (we'll factor later)\ncat_vars &lt;- c(\n  \"bed_type\",\"property_type\",\"neighbourhood_group_cleansed\",\"room_type\",\n  \"cancellation_policy\",\"host_is_superhost\",\"instant_bookable\",\n  \"zipcode\",\"city\",\"license\",\"jurisdiction_names\"\n)\nfor (v in intersect(cat_vars, names(baseData))) baseData[[v]] &lt;- as.character(baseData[[v]])\n\n# 1) Beds imputation via accommodates (simple linear model)\nif (all(c(\"beds\",\"accommodates\") %in% names(baseData))) {\n  fit_df &lt;- baseData %&gt;% dplyr::filter(!is.na(beds), !is.na(accommodates))\n  if (nrow(fit_df) &gt; 10) {\n    lm_beds &lt;- lm(beds ~ accommodates, data = fit_df)\n    baseData &lt;- baseData %&gt;%\n      dplyr::mutate(\n        beds = dplyr::if_else(\n          is.na(beds) & !is.na(accommodates),\n          round(predict(lm_beds, newdata = dplyr::pick(everything())), 0),\n          beds\n        )\n      )\n  }\n  if (any(is.na(baseData$beds))) {\n    baseData &lt;- baseData %&gt;% dplyr::mutate(beds = dplyr::if_else(is.na(beds), stats::median(beds, na.rm = TRUE), beds))\n  }\n}\n\n# 2) Zipcode impute to mode\nif (\"zipcode\" %in% names(baseData)) {\n  zip_mode &lt;- baseData %&gt;%\n    dplyr::filter(!is.na(zipcode)) %&gt;%\n    dplyr::count(zipcode, sort = TRUE) %&gt;%\n    dplyr::slice_head(n = 1) %&gt;%\n    dplyr::pull(zipcode)\n  baseData &lt;- baseData %&gt;% dplyr::mutate(zipcode = tidyr::replace_na(zipcode, zip_mode))\n}\n\n# 3) Amenities count\nif (all(c(\"id\",\"amenities\") %in% names(baseData))) {\n  amen_counts &lt;- baseData %&gt;%\n    dplyr::select(id, amenities) %&gt;%\n    dplyr::mutate(amenities = tidyr::replace_na(amenities, \"\")) %&gt;%\n    tidyr::separate_rows(amenities, sep = \",\") %&gt;%\n    dplyr::mutate(amenities = stringr::str_trim(amenities)) %&gt;%\n    dplyr::filter(amenities != \"\") %&gt;%\n    dplyr::count(id, name = \"count_amenities\")\n  baseData &lt;- baseData %&gt;%\n    dplyr::left_join(amen_counts, by = \"id\") %&gt;%\n    dplyr::mutate(count_amenities = tidyr::replace_na(count_amenities, 0L))\n}\n\n# 4) Normalized review score (0–5 from 0–100)\nbaseData &lt;- baseData %&gt;%\n  dplyr::mutate(review_score_5 = if (\"review_scores_rating\" %in% names(.)) review_scores_rating/20 else NA_real_)\n\n# Re-split to keep row roles\ntrain &lt;- baseData %&gt;% dplyr::filter(train_test_score == \"train\")\ntest  &lt;- baseData %&gt;% dplyr::filter(train_test_score == \"test\")\nscore &lt;- baseData %&gt;% dplyr::filter(train_test_score == \"score\") %&gt;% { if (nrow(.) == 0) NULL else . }\n\n# 5) Zipcode aggregates (TRAIN ONLY) to avoid leakage\nif (all(c(\"zipcode\",\"price\") %in% names(train))) {\n  zip_aggs &lt;- train %&gt;%\n    dplyr::group_by(zipcode) %&gt;%\n    dplyr::summarise(\n      zip_price_median = stats::median(price, na.rm = TRUE),\n      zip_price_mean   = mean(price, na.rm = TRUE),\n      .groups = \"drop\"\n    )\n  add_zip &lt;- function(df) {\n    out &lt;- df %&gt;% dplyr::left_join(zip_aggs, by = \"zipcode\")\n    out %&gt;%\n      dplyr::mutate(\n        zip_price_median = tidyr::replace_na(zip_price_median, stats::median(train$price, na.rm = TRUE)),\n        zip_price_mean   = tidyr::replace_na(zip_price_mean,   mean(train$price, na.rm = TRUE))\n      )\n  }\n  train &lt;- add_zip(train)\n  test  &lt;- add_zip(test)\n  if (!is.null(score)) score &lt;- add_zip(score)\n}"
  },
  {
    "objectID": "about.html#exploratory-data-analysis-eda",
    "href": "about.html#exploratory-data-analysis-eda",
    "title": "Predicting Airbnb Rental Prices in NYC",
    "section": "3. Exploratory Data Analysis (EDA)",
    "text": "3. Exploratory Data Analysis (EDA)\nPrice distributions show heavy right skew, which motivates log-transformation during modeling. Room type and capacity also show clear price separation.\n\n\nCode\np1 &lt;- ggplot(train, aes(price)) +\ngeom_histogram(bins = 50) +\nscale_x_continuous(labels = fmt_dollar) +\nlabs(title = \"Price distribution\", x = \"Price (USD)\", y = \"Count\")\n\np2 &lt;- ggplot(train, aes(log1p(price))) +\ngeom_histogram(bins = 50) +\nlabs(title = \"Log-price distribution\", x = \"log1p(Price)\", y = \"Count\")\n\np3 &lt;- train %&gt;%\nfilter(!is.na(room_type)) %&gt;%\nggplot(aes(x = room_type, y = price)) +\ngeom_boxplot(outlier.alpha = 0.2) +\nscale_y_continuous(labels = fmt_dollar) +\nlabs(title = \"Price by room type\", x = \"\", y = \"Price (USD)\")\n\np4 &lt;- ggplot(train, aes(x = accommodates, y = price)) +\ngeom_point(size = 0.9) +\ngeom_smooth(se = FALSE) +\nscale_y_continuous(labels = fmt_dollar) +\nlabs(title = \"Price vs. accommodates\", x = \"Accommodates\", y = \"Price (USD)\")\n\n(p1 | p2) / (p3 | p4)"
  },
  {
    "objectID": "about.html#modeling",
    "href": "about.html#modeling",
    "title": "Predicting Airbnb Rental Prices in NYC",
    "section": "4. Modeling",
    "text": "4. Modeling\nModels are trained on log(price) to stabilize variance and penalize large errors. Categorical variables are one-hot encoded and numeric variables are median-imputed.\nModels trained:\n\n\n\n\n\n\n\n\nModel\nLibrary\nNotes\n\n\n\n\nRandom Forest\nranger\nNon-linear, robust, interpretable feature importance\n\n\nElastic Net\nglmnet\nLinear, with L1/L2 regularization\n\n\nXGBoost\nxgboost\nGradient boosting, strong performance on tabular data\n\n\n\n\n\nCode\n# ---- Target on log scale ----\n\ntrain &lt;- train %&gt;% mutate(price_log = log1p(price))\ntest  &lt;- test  %&gt;% mutate(price_log = log1p(price))\n\n# Predictors (keep those present)\n\npredictors &lt;- c(\"host_is_superhost\",\"beds\",\"count_amenities\",\"neighbourhood_group_cleansed\",\"room_type\",\n\"accommodates\",\"bathrooms\",\"bedrooms\",\"cancellation_policy\",\"review_score_5\",\n\"number_of_reviews\",\"property_type\",\"instant_bookable\",\n\"zip_price_median\",\"zip_price_mean\",\"city\",\"zipcode\")\npredictors &lt;- intersect(predictors, names(train))\n\n# Numeric vs categorical\n\nis_num &lt;- vapply(train[, predictors, drop = FALSE], is.numeric, logical(1))\nnum_vars &lt;- names(is_num[is_num])\ncat_vars_model &lt;- setdiff(predictors, num_vars)\n\n# Numeric: median-impute using TRAIN\n\nnum_medians &lt;- purrr::map_dbl(num_vars, ~ median(train[[.x]], na.rm = TRUE))\nfor (v in num_vars) {\nif (anyNA(train[[v]])) train[[v]][is.na(train[[v]])] &lt;- num_medians[[v]]\nif (anyNA(test[[v]]))  test[[v]][is.na(test[[v]])]  &lt;- num_medians[[v]]\n}\n\n# Categorical: NA -&gt; \"Unknown\"; rare (&lt;1%) -&gt; \"Other\" based on TRAIN\n\npool_rare &lt;- function(x_train, x_new = NULL, thresh = 0.01) {\ntx &lt;- ifelse(is.na(x_train), \"Unknown\", as.character(x_train))\nprops &lt;- prop.table(table(tx))\nkeep &lt;- names(props[props &gt;= thresh])\nmap_levels &lt;- function(z) {\nz &lt;- ifelse(is.na(z), \"Unknown\", as.character(z))\nz[!(z %in% keep) & z != \"Unknown\"] &lt;- \"Other\"\nfactor(z, levels = unique(c(keep, \"Other\", \"Unknown\")))\n}\nlist(train = map_levels(tx), new = if (!is.null(x_new)) map_levels(x_new) else NULL)\n}\nfor (v in cat_vars_model) {\nres &lt;- pool_rare(train[[v]], test[[v]], thresh = 0.01)\ntrain[[v]] &lt;- res$train; test[[v]] &lt;- res$new\n}\nfor (v in cat_vars_model) {\ntrain[[v]] &lt;- as.factor(train[[v]])\ntest[[v]]  &lt;- factor(test[[v]], levels = levels(train[[v]]))\n}\n\n# Model matrices (one-hot)\n\ndf_train &lt;- train %&gt;% dplyr::select(all_of(c(\"price_log\", predictors)))\ndf_test  &lt;- test  %&gt;% dplyr::select(all_of(c(\"price_log\", predictors)))\n\nX_train &lt;- model.matrix(price_log ~ . - 1, data = df_train)\nX_test  &lt;- model.matrix(price_log ~ . - 1, data = df_test)\n\n# Align columns\n\nmissing_in_test &lt;- setdiff(colnames(X_train), colnames(X_test))\nif (length(missing_in_test)) {\nX_test &lt;- cbind(X_test, matrix(0, nrow = nrow(X_test), ncol = length(missing_in_test),\ndimnames = list(NULL, missing_in_test)))\n}\nextra_in_test &lt;- setdiff(colnames(X_test), colnames(X_train))\nif (length(extra_in_test)) {\nX_test &lt;- X_test[, setdiff(colnames(X_test), extra_in_test), drop = FALSE]\n}\nX_test &lt;- X_test[, colnames(X_train), drop = FALSE]\n\n# Targets\n\ny_train &lt;- df_train$price_log\ny_test  &lt;- df_test$price_log\n\n# CV control (used only if you retrain later)\n\ncv_ctrl &lt;- caret::trainControl(method = \"repeatedcv\", number = 5, repeats = 2, verboseIter = FALSE, allowParallel = TRUE)\n\n\n\nRandom Forest\n\n\nCode\n# ### ---------------- Random Forest (caret + ranger, saved model) ----------------\n# if (!dir.exists(\"models\")) dir.create(\"models\")\n# model_path_rf &lt;- \"models/rf_fit.rds\"\n# \n# if (file.exists(model_path_rf)) {\n#   message(\"Loading saved Random Forest model...\")\n#   rf_fit &lt;- readRDS(model_path_rf)\n# } else {\n#   message(\"Training Random Forest model (first time)...\")\n#   \n#   p &lt;- ncol(X_train)\n#   rf_grid &lt;- expand.grid(\n#     mtry          = unique(pmax(2, floor(c(sqrt(p), sqrt(p)*1.5, sqrt(p)*2)))),\n#     splitrule     = c(\"variance\", \"extratrees\"),\n#     min.node.size = c(3, 5, 10)\n#   )\n# \n#   set.seed(5656)\n#   rf_fit &lt;- caret::train(\n#     x = X_train,\n#     y = y_train,\n#     method      = \"ranger\",\n#     trControl   = cv_ctrl,\n#     tuneGrid    = rf_grid,\n#     num.trees   = 1000,\n#     importance  = \"impurity\",\n#     metric      = \"RMSE\"\n#   )\n#   \n#   saveRDS(rf_fit, model_path_rf)\n#   message(\"Saved Random Forest model to \", model_path_rf)\n# }\n# \n# # ---- Evaluate on test set ----\n# rf_pred_log   &lt;- predict(model_path_rf, newdata = X_test)\n# rf_pred_price &lt;- pmax(expm1(rf_pred_log), 0)\n# \n# # yardstick metrics if available; else caret::postResample\n# if (requireNamespace(\"yardstick\", quietly = TRUE)) {\n#   rf_metrics &lt;- tibble::tibble(price = expm1(y_test), .pred = rf_pred_price) |&gt;\n#     yardstick::metrics(truth = price, estimate = .pred) |&gt;\n#     dplyr::filter(.metric %in% c(\"rmse\",\"rsq\"))\n# } else {\n#   pm &lt;- caret::postResample(pred = rf_pred_price, obs = expm1(y_test))\n#   rf_metrics &lt;- data.frame(.metric = c(\"rmse\",\"rsq\"),\n#                            .estimate = c(unname(pm[\"RMSE\"]), unname(pm[\"Rsquared\"])))\n# }\n# rf_metrics\n\n\n\n\nElastic Net\n\n\nCode\n# ### ---------------- Elastic Net (caret + glmnet, saved model) ----------------\n# if (!dir.exists(\"models\")) dir.create(\"models\")\n# model_path_en &lt;- \"models/en_fit.rds\"\n# \n# if (file.exists(model_path_en)) {\n#   message(\"Loading saved Elastic Net model...\")\n#   en_fit &lt;- readRDS(model_path_en)\n# } else {\n#   message(\"Training Elastic Net model (first time)...\")\n#   \n#   # Tune across a broad but sensible grid\n#   en_grid &lt;- expand.grid(\n#     alpha  = seq(0, 1, by = 0.25),                    # ridge (0) -&gt; lasso (1)\n#     lambda = 10 ^ seq(2, -4, length.out = 50)         # regularization strength\n#   )\n#   \n#   set.seed(5656)\n#   en_fit &lt;- caret::train(\n#     x = X_train,\n#     y = y_train,\n#     method      = \"glmnet\",\n#     trControl   = cv_ctrl,        # 5x2 repeated CV from your prep\n#     tuneGrid    = en_grid,\n#     standardize = TRUE            # glmnet standardization (recommended)\n#     # preProcess = c(\"center\",\"scale\")  # optional, but glmnet standardizes internally\n#   )\n#   \n#   saveRDS(en_fit, model_path_en)\n#   message(\"Saved Elastic Net model to \", model_path_en)\n# }\n# \n# # ---- Evaluate on test set ----\n# en_pred_log   &lt;- predict(en_fit, newdata = X_test)\n# en_pred_price &lt;- pmax(expm1(en_pred_log), 0)\n# \n# # yardstick metrics if available; else caret::postResample\n# if (requireNamespace(\"yardstick\", quietly = TRUE)) {\n#   en_metrics &lt;- tibble::tibble(price = expm1(y_test), .pred = en_pred_price) |&gt;\n#     yardstick::metrics(truth = price, estimate = .pred) |&gt;\n#     dplyr::filter(.metric %in% c(\"rmse\",\"rsq\"))\n# } else {\n#   pm &lt;- caret::postResample(pred = en_pred_price, obs = expm1(y_test))\n#   en_metrics &lt;- data.frame(.metric = c(\"rmse\",\"rsq\"),\n#                            .estimate = c(unname(pm[\"RMSE\"]), unname(pm[\"Rsquared\"])))\n# }\n# en_metrics\n\n# ---- (Optional) Top coefficients for portfolio plot ----\n# Extract coefficients at best alpha/lambda\n# best_a &lt;- en_fit$bestTune$alpha\n# best_l &lt;- en_fit$bestTune$lambda\n# coefs  &lt;- as.matrix(coef(en_fit$finalModel, s = best_l))\n# coef_tbl &lt;- tibble::tibble(\n#   term = rownames(coefs),\n#   estimate = as.numeric(coefs)\n# ) |&gt;\n#   dplyr::filter(term != \"(Intercept)\") |&gt;\n#   dplyr::arrange(dplyr::desc(abs(estimate)))\n# \n\n\n\n\nXGBoost\n\n\nCode\n# ### ---------------- XGBoost (caret + xgboost, saved model) ----------------\n# if (!dir.exists(\"models\")) dir.create(\"models\")\n# model_path_xgb &lt;- \"models/xgb_fit.rds\"\n# \n# if (file.exists(model_path_xgb)) {\n#   message(\"Loading saved XGBoost model...\")\n#   xgb_fit &lt;- readRDS(model_path_xgb)\n# } else {\n#   message(\"Training XGBoost model (first time)...\")\n#   \n#   # caret::xgbTree tuning grid (balanced for speed/quality)\n#   xgb_grid &lt;- expand.grid(\n#     nrounds = c(400, 800, 1200),     # boosting iterations\n#     max_depth = c(4, 6, 8),          # tree depth\n#     eta = c(0.03, 0.1),              # learning rate\n#     gamma = c(0, 1),                 # minimum loss reduction\n#     colsample_bytree = c(0.6, 0.8),  # column subsample\n#     min_child_weight = c(1, 3),      # min sum hessian in child\n#     subsample = c(0.7, 1.0)          # row subsample\n#   )\n# \n#   set.seed(5656)\n#   xgb_fit &lt;- caret::train(\n#     x = X_train,\n#     y = y_train,\n#     method    = \"xgbTree\",\n#     trControl = cv_ctrl,        # your 5x2 repeated CV\n#     tuneGrid  = xgb_grid,\n#     metric    = \"RMSE\",\n#     verbose   = FALSE\n#   )\n#   \n#   saveRDS(xgb_fit, model_path_xgb)\n#   message(\"Saved XGBoost model to \", model_path_xgb)\n# }\n# \n# # ---- Evaluate on test set ---- \n# xgb_pred_log   &lt;- predict(xgb_fit, newdata = X_test)\n# xgb_pred_price &lt;- pmax(expm1(xgb_pred_log), 0)\n# \n# # yardstick if available; else use caret::postResample\n# if (requireNamespace(\"yardstick\", quietly = TRUE)) {\n#   xgb_metrics &lt;- tibble::tibble(price = expm1(y_test), .pred = xgb_pred_price) |&gt;\n#     yardstick::metrics(truth = price, estimate = .pred) |&gt;\n#     dplyr::filter(.metric %in% c(\"rmse\",\"rsq\"))\n# } else {\n#   pm &lt;- caret::postResample(pred = xgb_pred_price, obs = expm1(y_test))\n#   xgb_metrics &lt;- data.frame(.metric = c(\"rmse\",\"rsq\"),\n#                             .estimate = c(unname(pm[\"RMSE\"]), unname(pm[\"Rsquared\"])))\n# }\n# xgb_metrics\n# \n# # ---- (Optional) Feature importance (top 20) ----\n# # Uses the underlying xgboost booster inside the caret model\n# if (requireNamespace(\"xgboost\", quietly = TRUE)) {\n#   imp &lt;- xgboost::xgb.importance(feature_names = colnames(X_train),\n#                                  model = xgb_fit$finalModel)\n# }\n\n\n\nLoad Saved Models\n\n\nCode\n# ---- Load saved models from models/ ----\n\nmodel_path_rf  &lt;- \"models/rf_fit.rds\"\nmodel_path_en  &lt;- \"models/en_fit.rds\"\nmodel_path_xgb &lt;- \"models/xgb_fit.rds\"\n\nrf_fit  &lt;- if (file.exists(model_path_rf))  readRDS(model_path_rf)  else NULL\nen_fit  &lt;- if (file.exists(model_path_en))  readRDS(model_path_en)  else NULL\nxgb_fit &lt;- if (file.exists(model_path_xgb)) readRDS(model_path_xgb) else NULL\n\n# Helpers\n\nbt &lt;- function(x) pmax(expm1(x), 0)  # back-transform from log1p\neval_model &lt;- function(fit, X_new, y_true_log) {\nstopifnot(!is.null(fit))\npred_log &lt;- predict(fit, newdata = X_new)\npred     &lt;- bt(as.numeric(pred_log))\nobs      &lt;- bt(as.numeric(y_true_log))\npreds_df &lt;- tibble::tibble(price = obs, .pred = pred, resid = obs - pred)\n\nif (requireNamespace(\"yardstick\", quietly = TRUE)) {\nmets &lt;- preds_df |&gt;\nyardstick::metrics(truth = price, estimate = .pred) |&gt;\ndplyr::filter(.metric %in% c(\"rmse\",\"rsq\"))\n} else {\npm &lt;- caret::postResample(pred = preds_df$.pred, obs = preds_df$price)\nmets &lt;- tibble::tibble(.metric = c(\"rmse\",\"rsq\"),\n.estimate = c(unname(pm[\"RMSE\"]), unname(pm[\"Rsquared\"])))\n}\nlist(preds = preds_df, metrics = mets)\n}"
  },
  {
    "objectID": "about.html#model-comparison-selection",
    "href": "about.html#model-comparison-selection",
    "title": "Predicting Airbnb Rental Prices in NYC",
    "section": "5. Model Comparison & Selection",
    "text": "5. Model Comparison & Selection\n\n\nCode\nbt &lt;- function(x) pmax(expm1(x), 0)\n\n# Align a data.frame version of X to the model's required predictor names\nalign_newdata &lt;- function(fit, X) {\n  df &lt;- as.data.frame(X)\n  # Try to get the predictor names the model expects\n  req &lt;- NULL\n  # ranger via caret\n  if (!is.null(fit$finalModel) && inherits(fit$finalModel, \"ranger\")) {\n    req &lt;- try(fit$finalModel$forest$independent.variable.names, silent = TRUE)\n  }\n  # generic caret fallback (uses trainingData columns, minus .outcome)\n  if (is.null(req) || inherits(req, \"try-error\")) {\n    if (!is.null(fit$trainingData)) {\n      req &lt;- setdiff(colnames(fit$trainingData), \".outcome\")\n    }\n  }\n  # Final fallback: just use colnames(X) (shouldn't happen, but keeps robustness)\n  if (is.null(req)) req &lt;- colnames(df)\n\n  # Add missing as zeros, drop extras, order to req\n  miss &lt;- setdiff(req, colnames(df))\n  if (length(miss)) for (m in miss) df[[m]] &lt;- 0\n  extra &lt;- setdiff(colnames(df), req)\n  if (length(extra)) df &lt;- df[, setdiff(colnames(df), extra), drop = FALSE]\n  df[, req, drop = FALSE]\n}\n\n# Robust evaluator: try aligned X first; then raw test df as fallback\neval_model &lt;- function(fit, X_test, y_test, test_df) {\n  pred_log &lt;- NULL\n\n  nd &lt;- try(align_newdata(fit, X_test), silent = TRUE)\n  if (!inherits(nd, \"try-error\")) {\n    pred_log &lt;- try(predict(fit, newdata = nd), silent = TRUE)\n  }\n  if (inherits(pred_log, \"try-error\") || is.null(pred_log)) {\n    pred_log &lt;- try(predict(fit, newdata = test_df), silent = TRUE)\n  }\n  if (inherits(pred_log, \"try-error\") || is.null(pred_log)) return(NULL)\n\n  preds &lt;- tibble::tibble(price = expm1(y_test), .pred = bt(pred_log))\n  list(\n    preds   = preds,\n    metrics = preds |&gt;\n      yardstick::metrics(truth = price, estimate = .pred) |&gt;\n      dplyr::filter(.metric %in% c(\"rmse\",\"rsq\"))\n  )\n}\n\n# ---- Evaluate whatever models are available ----\nresults &lt;- list()\nif (exists(\"rf_fit\")  && !is.null(rf_fit))  results$RF  &lt;- eval_model(rf_fit,  X_test, y_test, test)\nif (exists(\"en_fit\")  && !is.null(en_fit))  results$EN  &lt;- eval_model(en_fit,  X_test, y_test, test)\nif (exists(\"xgb_fit\") && !is.null(xgb_fit)) results$XGB &lt;- eval_model(xgb_fit, X_test, y_test, test)\n\n# Keep only successful evaluations\nresults &lt;- purrr::compact(results)\nstopifnot(length(results) &gt; 0)\n\n# ---- Build comparison table ----\nmetrics_tbl &lt;- purrr::imap_dfr(results, ~ .x$metrics %&gt;% dplyr::mutate(model = .y)) %&gt;%\n  dplyr::select(model, .metric, .estimate) %&gt;%\n  dplyr::mutate(.metric = tolower(.metric)) %&gt;%\n  tidyr::pivot_wider(names_from = .metric, values_from = .estimate) %&gt;%\n  dplyr::mutate(\n    rmse = as.numeric(rmse),\n    rsq  = as.numeric(rsq)\n  ) %&gt;%\n  dplyr::arrange(rmse)\n\n# Optional pretty table\nmetrics_tbl %&gt;%\n  gt::gt() %&gt;%\n  gt::tab_header(title = \"Model Performance on Test Set\") %&gt;%\n  gt::cols_label(rmse = \"RMSE ($)\", rsq = \"R²\") %&gt;%\n  gt::tab_style(style = gt::cell_text(weight = \"bold\"),\n                locations = gt::cells_column_labels())\n\n\n\n\n\n\n\n\nModel Performance on Test Set\n\n\nmodel\nRMSE ($)\nR²\n\n\n\n\nXGB\n65.65469\n0.6414969\n\n\nRF\n66.96682\n0.6360707\n\n\nEN\n70.17521\n0.5860496\n\n\n\n\n\n\n\nCode\n# ---- Pick best & prep for visuals ----\nbest_name &lt;- metrics_tbl$model[1]\nbest_preds &lt;- results[[best_name]]$preds %&gt;%\n  dplyr::transmute(price, pred = .pred, resid = price - .pred)\n\nmessage(sprintf(\"✅ Best model: %s — RMSE $%.1f | R² %.3f\",\n                best_name, metrics_tbl$rmse[1], metrics_tbl$rsq[1]))\n\nbest_name &lt;- as.character(metrics_tbl$model[1])\n\n# Map the winning label -&gt; actual model object\nbest_fit &lt;- switch(best_name,\n  RF  = rf_fit,\n  EN  = en_fit,\n  XGB = xgb_fit\n)\nstopifnot(!is.null(best_fit))\n\n\nThe table shows performance on the held-out test set.\nLower RMSE = better. Higher R² = better.\nThe best-performing model is XGB, with:\n\nRMSE: $65.70\nR²: 0.641\n\nThis model is used for all diagnostics and predictions."
  },
  {
    "objectID": "about.html#model-diagnostics",
    "href": "about.html#model-diagnostics",
    "title": "Predicting Airbnb Rental Prices in NYC",
    "section": "6. Model Diagnostics",
    "text": "6. Model Diagnostics\nInterpretation of Diagnostic Plots\nPredicted vs Actual: The XGBoost model closely tracks the 45° reference line, indicating good predictive accuracy. Most points cluster in the lower–mid price range, with some under-prediction at higher prices — a common pattern when extreme nightly rates are rare in the training data.\nResiduals vs Predicted: Residuals are centered around zero with no strong patterns, suggesting the model is not systematically over- or under-predicting across most price ranges. A few large positive and negative residuals indicate outliers (listings with unusually high or low prices).\nResiduals Q–Q Plot: The residual distribution deviates from perfect normality in the tails — again driven by extreme listings — but the majority fall near the reference line, supporting that model errors are approximately symmetric and well-behaved.\nOverall, the diagnostics show that the XGBoost model generalizes well, captures the main pricing structure in the data, and only struggles on a small number of extreme cases (luxury or atypical listings).\n\n\nCode\n# --- prerequisites ---\nif (!exists(\"metrics_tbl\")) stop(\"metrics_tbl not found.\")\nif (!exists(\"results\"))     stop(\"results not found.\")\nif (!requireNamespace(\"hexbin\", quietly = TRUE)) install.packages(\"hexbin\")\nif (!requireNamespace(\"patchwork\", quietly = TRUE)) install.packages(\"patchwork\")\nlibrary(patchwork)\n\n# pick best model (lowercase metric names)\nbest_row  &lt;- metrics_tbl %&gt;% dplyr::slice_min(rmse, n = 1)\nbest_name &lt;- as.character(best_row$model)\n\n# (re)build best_preds if needed\nif (!exists(\"best_preds\") || is.null(best_preds)) {\n  if (!is.null(results[[best_name]]) && !is.null(results[[best_name]]$preds)) {\n    best_preds &lt;- results[[best_name]]$preds %&gt;%\n      dplyr::transmute(price, pred = .pred, resid = price - .pred)\n  } else {\n    if (!exists(\"bt\")) bt &lt;- function(x) pmax(expm1(x), 0)\n    pred_log &lt;- predict(switch(best_name, RF=rf_fit, EN=en_fit, XGB=xgb_fit), newdata = X_test)\n    best_preds &lt;- tibble::tibble(\n      price = bt(as.numeric(y_test)),\n      pred  = bt(as.numeric(pred_log))\n    ) %&gt;% dplyr::mutate(resid = price - pred)\n  }\n}\n\n# helpers\nif (!exists(\"fmt_dollar\")) fmt_dollar &lt;- scales::label_dollar(accuracy = 1, prefix = \"$\", big.mark = \",\")\n\n# set symmetric residual limits (trim extreme outliers for readability)\nr_lim &lt;- stats::quantile(abs(best_preds$resid), 0.99, na.rm = TRUE) |&gt; as.numeric()\n\n# --- Plot A: Predicted vs Actual (bigger at top) ---\npA &lt;- ggplot(best_preds, aes(x = price, y = pred)) +\n  geom_hex(bins = 35, show.legend = TRUE) +\n  scale_fill_viridis_c(name = \"Count\") +\n  geom_abline(slope = 1, intercept = 0, linetype = 2, linewidth = 0.7) +\n  coord_equal() +\n  scale_x_continuous(labels = fmt_dollar, breaks = scales::breaks_pretty(6)) +\n  scale_y_continuous(labels = fmt_dollar, breaks = scales::breaks_pretty(6)) +\n  labs(title = sprintf(\"%s — Predicted vs Actual\", best_name),\n       x = \"Actual price\", y = \"Predicted price\") +\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    legend.position = \"right\"\n  )\n\n# --- Plot B: Residuals vs Predicted ---\npB &lt;- ggplot(best_preds, aes(x = pred, y = resid)) +\n  geom_point(alpha = 0.25, size = 0.8) +\n  geom_smooth(method = \"loess\", se = FALSE, linewidth = 0.9) +\n  geom_hline(yintercept = 0, linetype = 2, linewidth = 0.6) +\n  scale_x_continuous(labels = fmt_dollar, breaks = scales::breaks_pretty(5)) +\n  scale_y_continuous(limits = c(-r_lim, r_lim)) +\n  labs(title = \"Residuals vs Predicted\",\n       x = \"Predicted price\", y = \"Residual\") +\n  theme_minimal(base_size = 12) +\n  theme(plot.title = element_text(face = \"bold\"))\n\n# --- Plot C: Residuals Q–Q Plot ---\npC &lt;- ggplot(best_preds, aes(sample = resid)) +\n  stat_qq(alpha = 0.5, size = 1) +\n  stat_qq_line(color = \"red\", linewidth = 0.8) +\n  labs(title = \"Residuals — Q–Q Plot\",\n       x = \"Theoretical Quantiles\", y = \"Sample Quantiles\") +\n  theme_minimal(base_size = 12) +\n  theme(plot.title = element_text(face = \"bold\"))\n# Add subtle margins so labels never collide with the panel edge\npA &lt;- pA + theme(plot.margin = margin(6, 12, 6, 6))\npB &lt;- pB + theme(plot.margin = margin(6, 12, 6, 6))\npC &lt;- pC + theme(plot.margin = margin(6, 6, 6, 12))\n\n# Build a spacer to force separation\nsp &lt;- patchwork::plot_spacer()\n\n# Bottom row with a spacer column in the middle\nbottom_row &lt;- (pB | sp | pC) + patchwork::plot_layout(widths = c(1, 0.08, 1))\n\n# Final layout: large top, separated bottoms\nfinal_diag &lt;- (pA) / bottom_row +\n  patchwork::plot_layout(heights = c(1.35, 1), guides = \"collect\") &\n  theme(legend.position = \"right\")\n\nfinal_diag"
  },
  {
    "objectID": "about.html#feature-importance",
    "href": "about.html#feature-importance",
    "title": "Predicting Airbnb Rental Prices in NYC",
    "section": "7. Feature Importance",
    "text": "7. Feature Importance\n\n\nCode\n# Ensure best_fit exists even if this chunk is run alone\nif (!exists(\"best_fit\") || is.null(best_fit)) {\n  best_fit &lt;- switch(best_name, RF = rf_fit, EN = en_fit, XGB = xgb_fit)\n  stopifnot(!is.null(best_fit))\n}\n\nif (best_name == \"RF\") {\n\n# caret::varImp works on the trained ranger model wrapped by caret\n\nvi &lt;- caret::varImp(best_fit)$importance %&gt;%\ntibble::rownames_to_column(\"feature\") %&gt;%\ndplyr::arrange(dplyr::desc(Overall)) %&gt;%\ndplyr::slice(1:20)\nggplot(vi, aes(x = reorder(feature, Overall), y = Overall)) +\ngeom_col(fill = \"#4E79A7\") + coord_flip() +\nlabs(title = \"Top 20 Features — Random Forest\", x = \"\", y = \"Importance\")\n\n} else if (best_name == \"XGB\") {\n\n# Use underlying xgboost booster inside caret model\n\nif (requireNamespace(\"xgboost\", quietly = TRUE)) {\nimp &lt;- xgboost::xgb.importance(\nfeature_names = colnames(X_train),\nmodel = best_fit$finalModel\n)\nimp &lt;- imp %&gt;% dplyr::slice(1:20)\nggplot(imp, aes(x = reorder(Feature, Gain), y = Gain)) +\ngeom_col(fill = \"#E67E22\") + coord_flip() +\nlabs(title = \"Top 20 Features — XGBoost\", x = \"\", y = \"Gain\")\n}\n} else if (best_name == \"EN\") {\n\n# Coefficients from glmnet at best lambda\n\nbest_l &lt;- best_fit$bestTune$lambda\ncf &lt;- as.matrix(coef(best_fit$finalModel, s = best_l))\nen_coefs &lt;- tibble::tibble(term = rownames(cf), estimate = as.numeric(cf)) %&gt;%\ndplyr::filter(term != \"(Intercept)\") %&gt;%\ndplyr::arrange(dplyr::desc(abs(estimate))) %&gt;%\ndplyr::slice(1:20)\nggplot(en_coefs, aes(x = reorder(term, estimate), y = estimate)) +\ngeom_col(fill = \"#2E86C1\") + coord_flip() +\nlabs(title = \"Top 20 Elastic Net Coefficients\", x = \"\", y = \"Coefficient (log-price scale)\")\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nModel-specific interpretation notes\n\nXGBoost “Gain”\n\nGain tells us how important a feature is by measuring how much it helps the model reduce prediction error.\nHigher gain = the feature contributed more to improving accuracy.\n\nElastic Net coefficients\n\nCoefficients are learned on the log-price scale, which means they translate to percentage changes in price.\nTo interpret a coefficient: convert using exp(coef) - 1.\nExample: a coefficient of +0.07 ≈ +7.25% higher price, because exp(0.07) - 1 ≈ 0.0725."
  },
  {
    "objectID": "about.html#predicting-on-new-unseen-data",
    "href": "about.html#predicting-on-new-unseen-data",
    "title": "Predicting Airbnb Rental Prices in NYC",
    "section": "8. Predicting on New (Unseen) Data",
    "text": "8. Predicting on New (Unseen) Data\nAfter training and selecting the best model, predictions were generated for scoringData.csv, which contains new Airbnb listings with no recorded prices. The goal is to estimate their nightly rental cost for submission.\nThe training dataset reflects real NYC Airbnb market behavior. To understand market structure, I summarized and visualized the price distribution in the analysis dataset. The median nightly price is approximately $110, and most listings fall between $50–$250 per night, with a long right-tail driven by premium and luxury properties.\nAfter producing model predictions for the unseen listings, I compared their distribution to the observed market data.\nThe predicted density curve aligns closely with the historical market distribution, which indicates:\n\nthe model generalizes well,\npredictions fall within realistic market ranges, and\nthe model does not systematically over- or under-estimate prices.\n\nThis provides confidence that the model outputs plausible nightly price estimates for new listings.\n\n\nCode\nstopifnot(exists(\"best_fit\") && !is.null(best_fit))\nif (is.null(score)) stop(\"No scoring data found at 'data/scoringData.csv'.\")\n\n# Helper: get the exact feature names the trained model expects\nget_req_names &lt;- function(fit) {\n  # xgboost via caret stores names in finalModel$feature_names\n  nm &lt;- try(fit$finalModel$feature_names, silent = TRUE)\n  if (!inherits(nm, \"try-error\") && !is.null(nm)) return(as.character(nm))\n  # generic caret fallback: columns used during training (minus .outcome)\n  if (!is.null(fit$trainingData)) {\n    return(setdiff(colnames(fit$trainingData), \".outcome\"))\n  }\n  # last resort: use X_train colnames if present\n  if (exists(\"X_train\")) return(colnames(X_train))\n  stop(\"Could not infer required feature names for prediction.\")\n}\n\n# 1) Ensure we have a score matrix built (from prep-score-matrix)\nstopifnot(exists(\"X_score\"))\n\n# 2) Align X_score to the model’s required names\nreq &lt;- get_req_names(best_fit)\n\n# Start from current X_score; add missing zeros, drop extras, order columns\nX_sc &lt;- X_score\nmiss &lt;- setdiff(req, colnames(X_sc))\nif (length(miss)) {\n  X_sc &lt;- cbind(\n    X_sc,\n    matrix(0, nrow = nrow(X_sc), ncol = length(miss), dimnames = list(NULL, miss))\n  )\n}\nextra &lt;- setdiff(colnames(X_sc), req)\nif (length(extra)) X_sc &lt;- X_sc[, setdiff(colnames(X_sc), extra), drop = FALSE]\nX_sc &lt;- X_sc[, req, drop = FALSE]\n\n# Ensure it is a numeric matrix for xgboost\nX_sc &lt;- as.matrix(X_sc)\nstorage.mode(X_sc) &lt;- \"double\"\n\n# 3) Predict (matrix path first; fallback to data.frame if needed)\npred_log &lt;- try(predict(best_fit, newdata = X_sc), silent = TRUE)\nif (inherits(pred_log, \"try-error\") || is.null(pred_log)) {\n  pred_log &lt;- predict(best_fit, newdata = as.data.frame(X_sc))\n}\n\n# 4) Back-transform from log1p and clip at zero\npred_price &lt;- pmax(expm1(as.numeric(pred_log)), 0)\n\n# 5) Build submission file: ONLY id and price\nsubmission &lt;- score %&gt;% dplyr::select(id) %&gt;% dplyr::mutate(price = pred_price)\n\n# Sanity checks\nstopifnot(all(c(\"id\",\"price\") %in% names(submission)))\nstopifnot(!any(is.na(submission$price)))\nstopifnot(!any(submission$price &lt; 0))\n\n# 6) Write CSV per competition spec\nreadr::write_csv(submission, \"submission.csv\")\n\n# Preview\n# head(submission)\n\n\n\n\nCode\nlibrary(ggplot2)\nggplot(analysisData, aes(price)) +\n  geom_histogram(bins = 60, fill = \"pink\", alpha = 0.6) +\n  scale_x_continuous(labels = scales::dollar) +\n  labs(title = \"Distribution of Nightly Prices (Observed Market)\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nstopifnot(exists(\"submission\"), exists(\"train\"))\npreds_df &lt;- submission %&gt;% dplyr::mutate(source = \"Predicted (Scoring)\")\ntrain_df &lt;- train %&gt;% dplyr::transmute(price, source = \"Observed (Train)\")\n\nggplot(dplyr::bind_rows(\n        preds_df %&gt;% dplyr::rename(price = price),\n        train_df),\n       aes(x = price, fill = source)) +\n  geom_density(alpha = 0.4) +\n  scale_x_continuous(labels = fmt_dollar) +\n  labs(title = \"Where do predicted prices sit vs. the training market?\",\n       x = \"Nightly price\", y = \"Density\", fill = \"\") +\n  theme_minimal()"
  },
  {
    "objectID": "about.html#conclusion",
    "href": "about.html#conclusion",
    "title": "Predicting Airbnb Rental Prices in NYC",
    "section": "8. Conclusion",
    "text": "8. Conclusion\nThis project built a complete machine-learning workflow to predict nightly Airbnb rental prices in New York City — from data cleaning and feature engineering to model training, evaluation, and pricing new listings.\nAfter evaluating model performance on a held-out test set, XGBoost achieved the strongest results, with the lowest RMSE and highest R². Random Forest and Elastic Net performed reasonably well, but XGBoost captured non-linear relationships in the pricing structure more effectively.\nKey takeaways:\n\nMajor pricing drivers included accommodates, room type, amenities, and zipcode-level price aggregates, all of which align with real-world market behavior.\nResidual diagnostics showed errors centered around zero with minimal skew, indicating that the model is not systematically over- or under-pricing listings.\nFeature importance confirmed economic intuition: larger spaces, premium neighborhoods, and richer amenities consistently lead to higher nightly rates.\nModel predictions closely matched actual market prices, with only a small number of high-end luxury listings producing wider residuals.\n\n\nScoring New Data\nA cleaned version of the scoring dataset was passed through the trained pipeline, including:\n\nmissing-value handling\n\ncategorical alignment\n\namenities engineering\n\nzipcode-level pricing features\n\nlog-to-price back-transformation\n\nFinal predictions were exported in the required Kaggle format (id, price) and saved as submission.csv.\nThe distribution of predicted prices closely overlapped the historical market data, suggesting realistic, market-consistent pricing.\n\n\nPotential Next Improvements\nEnhancements possible with the current dataset: - Engineer richer host reputation features (e.g., review volume tiers, normalized ratings, superhost indicators) - Build a two-stage modeling approach that first identifies luxury listings, then predicts price within each segment - Add additional zipcode-level market signals (e.g., median neighborhood price, price density)\nEnhancements requiring additional data: - Incorporate calendar effects such as weekends, holidays, or seasonal demand - Add spatial features such as distance to landmarks, subway access, or walkability - Evaluate more advanced ensemble methods (LightGBM, CatBoost, or stacked models)\n\n\nFinal Note\nWith a strong predictive model and a fully reproducible scoring pipeline, this approach can now price new Airbnb listings — valuable for hosts optimizing revenue, investors evaluating short-term rental markets, or platforms designing dynamic pricing tools."
  }
]